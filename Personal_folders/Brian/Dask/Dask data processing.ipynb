{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect to the Dask dashboard use ssh portforwarding: http://distributed.dask.org/en/latest/web.html#id1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "mpl.style.use('default')\n",
    "import glob\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster, progress, fire_and_forget\n",
    "from dask import delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up LocalCluster & run it\n",
    "Might not be needed, but experienced that `processes=False, n_workers=1` are the best options to have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster = LocalCluster(processes=False, n_workers=8)\n",
    "cluster = LocalCluster(processes=False, n_workers=1)\n",
    "#cpu_worker = cluster.workers[0]\n",
    "#cpu_worker.name = 'cpu'\n",
    "#cpu_worker.set_resources(CPU=90)\n",
    "\n",
    "client=Client(cluster, processes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b600cc3cba7f4d0d93aa0bf9c2c37503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>VBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "VBox(children=(HTML(value='<h2>LocalCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n    .dataframe tbody tr th:only-of-type {\\n        vertical-align: middle;\\n    }\\n\\n    .dataframe tbody tr th {\\n        vertical-align: top;\\n    }\\n\\n    .dataframe thead th {\\n        text-align: right;\\n    }\\n  </style>\\n  <table style=\"text-align: right;\">\\n    <tr><th>Workers</th> <td>1</td></tr>\\n    <tr><th>Cores</th> <td>8</td></tr>\\n    <tr><th>Memory</th> <td>17.02 GB</td></tr>\\n  </table>\\n</div>\\n', layout=Layout(min_width='150px')), Accordion(children=(HBox(children=(IntText(value=0, description='Workers', layout=Layout(width='150px')), Button(description='Scale', layout=Layout(width='150px'), style=ButtonStyle()))), HBox(children=(IntText(value=0, description='Minimum', layout=Layout(width='150px')), IntText(value=0, description='Maximum', layout=Layout(width='150px')), Button(description='Adapt', layout=Layout(width='150px'), style=ButtonStyle())))), layout=Layout(min_width='500px'), selected_index=None, _titles={'0': 'Manual Scaling', '1': 'Adaptive Scaling'}))), HTML(value='<p><b>Dashboard: </b><a href=\"http://145.52.169.135/8416/1:8787/status\" target=\"_blank\">http://145.52.169.135/8416/1:8787/status</a></p>\\n')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>inproc://145.52.169.135/8416/1\n",
       "  <li><b>Dashboard: </b><a href='http://localhost:8787/status' target='_blank'>http://localhost:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>17.02 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='inproc://145.52.169.135/8416/1' processes=1 cores=8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_weather_data():\n",
    "    \"\"\"\n",
    "    Reads in the weather Pandas DataFrame.\n",
    "    :return: Pandas DataFrame\n",
    "    \"\"\"\n",
    "    # Check if UTC to gmt+1 conversion is being handled correctly\n",
    "    weather = pd.read_csv('F://datc//opschaler//weather_data//knmi_10_min_raw_data//output//df_combined_uncleaned.csv',\n",
    "                          delimiter='\\t', comment='#',\n",
    "                          parse_dates=['datetime'])\n",
    "    weather = weather.set_index(['datetime'])\n",
    "    weather = weather.astype('float32')\n",
    "    return weather\n",
    "\n",
    "\n",
    "def smartmeter_data():\n",
    "    \"\"\"\n",
    "    Reads in the file paths and dwelling id's of the smartmeter data.\n",
    "    :return: file_paths, dwelling_ids, both as lists.\n",
    "    \"\"\"\n",
    "    path = 'F://datc//opschaler//smartmeter_data//'\n",
    "    file_paths = np.array(glob.glob(path + \"*.csv\"))\n",
    "\n",
    "    print('Detected %s smartmeter_data files.' % len(file_paths))\n",
    "    dwelling_ids = np.array(list((map(lambda x: x[-15:-4], file_paths))))\n",
    "\n",
    "    return file_paths, dwelling_ids\n",
    "\n",
    "\n",
    "def reduce_memory(df):\n",
    "    \"\"\"\n",
    "    Reduces memory footprint of the input dataframe.\n",
    "    Changes float64 columns to float32 dtype.\n",
    "    \"\"\"\n",
    "    columns = df.columns\n",
    "    memory_before = df.memory_usage(deep=False).sum() / 2**30 # convert bytes to GB\n",
    "\n",
    "    for column in columns:\n",
    "        if df[column].dtype == 'float64':\n",
    "            df[column] = df[column].astype('float32')\n",
    "        \n",
    "    #memory_after = df.memory_usage(deep=False).sum() / 2**30 # convert bytes to GB\n",
    "    #print('Memory uasge reduced from %.3f GB to %.3f GB' % (memory_before, memory_after))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "@delayed(nout=2)\n",
    "def clean_prepare_smart_gas(file_path, dwelling_id):\n",
    "    \"\"\"\n",
    "    Input is a dwelling_id.csv file.\n",
    "    Output are cleaned & prepared dataframes (smart, gas).\n",
    "\n",
    "    :param file_path: path to 'dwelling_id.csv' file\n",
    "    :return: Smart and gas Pandas DataFrames\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path, delimiter=';', header=0)\n",
    "    df = df.rename(index=str, columns={'Timestamp': 'datetime', 'gasTimestamp': 'datetime'})\n",
    "\n",
    "    smart = df.iloc[:, :7]\n",
    "    gas = df.iloc[:, 7:]\n",
    "    \n",
    "    del df\n",
    "    \n",
    "    try:\n",
    "        smart['datetime'] = pd.to_datetime(smart['datetime'])\n",
    "        gas['datetime'] = pd.to_datetime(gas['datetime'])\n",
    "    except:\n",
    "        print('datetime column contains non-datetime values')\n",
    "        smart = clean_datetime(smart)\n",
    "        gas = clean_datetime(gas)\n",
    "        smart['datetime'] = pd.to_datetime(smart['datetime'])\n",
    "        gas['datetime'] = pd.to_datetime(gas['datetime'])\n",
    "\n",
    "    smart = smart.set_index(['datetime'])\n",
    "    gas = gas.set_index(['datetime'])\n",
    "\n",
    "    smart = reduce_memory(smart)\n",
    "    gas = reduce_memory(gas)\n",
    "\n",
    "    return smart, gas\n",
    "\n",
    "\n",
    "@delayed\n",
    "def clean_datetime(df):\n",
    "    \"\"\"\n",
    "    TODO: Speed up the function\n",
    "    Input should be a df with a column called 'datetime'.\n",
    "    This function checks wether a row in the df.datetime column can be parsed to a Pandas datetime object,\n",
    "    by trying pd.to_datetime() on it.\n",
    "    If it fails it will replace that row with np.nan().\n",
    "    Finally this function will return the df with the NaN rows dropped.\n",
    "    It only drops the row if the datetime column contains a NaN.\n",
    "\n",
    "    :param df: Pandas DataFrame containing a datetime column called 'datetime'.\n",
    "    :return: Pandas DataFrame\n",
    "    \"\"\"\n",
    "    for i in range(len(df)):\n",
    "        try:\n",
    "            pd.to_datetime(df.datetime[i])\n",
    "        except ValueError:\n",
    "            print('-----')\n",
    "            print('ValueError at index = %s' % i)\n",
    "            print(df.datetime[i])\n",
    "            df.datetime = df.datetime.replace(df.datetime[i], np.nan)\n",
    "    df = df.dropna(subset=['datetime'])\n",
    "    return df\n",
    "\n",
    "\n",
    "@delayed(nout=2)\n",
    "def resample_dfs(smart, gas):\n",
    "    smart = smart.resample('10s').mean()\n",
    "    gas = gas.resample('H').mean()\n",
    "    \n",
    "    return smart, gas\n",
    "\n",
    "\n",
    "@delayed\n",
    "def create_hour_df(smart, gas, weather, dwelling_id):    \n",
    "    # resample to original sample rates, just to be sure\n",
    "    smart = smart.resample('10s').mean()\n",
    "    gas = gas.resample('H').mean()\n",
    "    \n",
    "    # resample smart df to one hour, define what to do per column\n",
    "    to_mean = ['ePower', 'ePowerReturn']\n",
    "    to_last = ['eMeter', 'eMeterReturn', 'eMeterLow', 'eMeterLowReturn']\n",
    "    \n",
    "    # Resample columns by said methods, this will not resample the index yet.\n",
    "    smart[to_mean] = smart[to_mean].resample('H').mean() # take mean of values\n",
    "    smart[to_last] = smart[to_last].resample('H').last() # take 'last' known value, see pandas documentation for more info\n",
    "    # ^ Vectors validates baldiri's wishes\n",
    "    \n",
    "    # Resample the complete df to H, including indices.\n",
    "    smart = smart.resample('H').mean()\n",
    "    \n",
    "    # Down sample weather df to one hour\n",
    "    weather = weather.resample('H').mean()\n",
    "    \n",
    "    # Combine gas, smart, weather into one df\n",
    "    df_hour = pd.merge(smart, gas, left_index=True, right_index=True)\n",
    "    df_hour = pd.merge(df_hour, weather, left_index=True, right_index=True)\n",
    "    \n",
    "    # Add a dwelling id column\n",
    "    df_hour['dwelling'] = dwelling_id\n",
    "    \n",
    "    return df_hour\n",
    "\n",
    "\n",
    "@delayed\n",
    "def create_10s_df(smart, gas, weather, dwelling_id):\n",
    "    gas = gas.resample('10s').ffill()  # Up sample gas to 10s by forward filling the values\n",
    "    # Calculate gasPower column, is this the right way? Or should we ffill it?\n",
    "    # Currently this code makes it so there is one gasPower value per hour\n",
    "    gas['gasPower'] = gas['gasMeter'].diff()\n",
    "\n",
    "    weather = weather.resample('10s').ffill()  # forward fill because the raw data is the 10 minute mean\n",
    "    \n",
    "    # Combine gas, smart, weather into one df\n",
    "    df_10s = pd.merge(smart, gas, left_index=True, right_index=True)\n",
    "    df_10s = pd.merge(df_10s, weather, left_index=True, right_index=True)\n",
    "    df_10s['dwelling'] = dwelling_id # add a dwelling id column\n",
    "    \n",
    "    return df_10s\n",
    "\n",
    "\n",
    "# Can't get this to work delayed because of object oriented stuff\n",
    "def plot_nans(df, dwelling_id, resample_to):\n",
    "    \"\"\"\n",
    "    Create a heatmap of the NaNs in the input DataFrame.\n",
    "    :param df: Pandas DataFrame\n",
    "    :param df: String to resample to, for example '1T' or 'H'\n",
    "    :param dwelling_id: String\n",
    "    :return: Seaborn heatmap as a Figure\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.isnull()\n",
    "    # Downsample to make all data visible\n",
    "    df = df.resample(resample_to).sum()  # Downsample to make small NaNs visible\n",
    "    df = df.apply(lambda x: x > 0, 1)  # Replace values >0 with 1\n",
    "\n",
    "    # Reindex datetimes\n",
    "    # https://stackoverflow.com/questions/41046630/set-time-formatting-on-a-datetime-index-when-plotting-pandas-series\n",
    "    try:\n",
    "        df.index = df.index.to_period('D')\n",
    "    except:\n",
    "        print('plot_nans could not set df.index.to_period')\n",
    "\n",
    "    # Plot heatmap\n",
    "    n = int(len(df)*0.1)  # Choose amount of yticklabels to show\n",
    "    \n",
    "    #fig = plt.figure(clear=True)\n",
    "    \n",
    "    try:\n",
    "        ax = sns.heatmap(df, cmap='Reds', square=False, vmin=0, cbar=False, yticklabels=n*2, cbar_kws={})\n",
    "    except TypeError:\n",
    "        print('plot_nans ValueError')\n",
    "        ax = sns.heatmap(df, cmap='Reds', square=False, vmin=0, cbar=False, cbar_kws={})\n",
    "\n",
    "    # Set cbar ticks manually\n",
    "    #cbar = fig.collections[0].colorbar\n",
    "    #cbar.set_ticks([0, 1])\n",
    "    #cbar.set_ticklabels(['Not NaN', 'NaN'])\n",
    "\n",
    "    # Correct layout\n",
    "    ax.invert_yaxis()\n",
    "    ax.tick_params(axis='x', rotation=90)\n",
    "    ax.tick_params(axis='y', rotation=0)\n",
    "    ax.set(xlabel='Column [-]', ylabel='Index [-]')\n",
    "    ax.set_title('Dwelling ID: '+dwelling_id)\n",
    "    \n",
    "    fig = ax.get_figure()\n",
    "    \n",
    "    #.tight_layout()\n",
    "    #fig.show()\n",
    "    #print('Saving heatmap')\n",
    "    #fig.savefig('F://datc//opschaler//nan_information//figures//' + dwelling_id + '.png', dpi=1200)\n",
    "    #savefig crashes dask\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "@delayed\n",
    "def df_nan_checker(df, threshold_percentage):\n",
    "    \"\"\"\n",
    "    TODO: Parellalize, as in one column per core/worker?\n",
    "    Checks each column in the input dataframe for NaNs.\n",
    "    Outputs the amount of NaNs behind each other, including the start and stop index, per column as a sublist.\n",
    "    For example when the dataframe has three columns.\n",
    "    Output is in the form of:\n",
    "    [[column_one_info], [column_two_info], [column_three_info]]\n",
    "    With the column_..._info being in the form of:\n",
    "    [start_index, stop_index, amount_of_NaNs]\n",
    "\n",
    "    :param df: Pandas DataFrame\n",
    "    :param threshold_percentage: Filter output based on NaN streaks being larger than x % of the total length of the dataframe.\n",
    "    :return: Pandas DataFrame\n",
    "    \"\"\"\n",
    "    columns = df.columns\n",
    "    df = df.isnull()\n",
    "    output = []\n",
    "    length = len(columns)\n",
    "    \n",
    "    \n",
    "    @delayed\n",
    "    def check_rows(df, column_name):\n",
    "        column_info = []\n",
    "        temp = []\n",
    "        x = False\n",
    "\n",
    "        for j, value in enumerate(df[column_name]):\n",
    "            if x == False and value == True:\n",
    "                temp.append(df.index[j])\n",
    "                x = True\n",
    "            elif x == True and value == True:\n",
    "                temp.append(df.index[j])\n",
    "            elif x == True and value == False:\n",
    "                column_info.append(temp)\n",
    "                temp = []\n",
    "                x = False\n",
    "\n",
    "        lengths = []\n",
    "\n",
    "        for array in column_info:\n",
    "            lengths.append([array[0], array[-1], len(array)])\n",
    "\n",
    "        return lengths\n",
    "\n",
    "    \n",
    "    for i in range(length):\n",
    "        lengths = check_rows(df, columns[i])\n",
    "        output.append(lengths)\n",
    "    \n",
    "    @delayed\n",
    "    def list_to_df(output):\n",
    "        # Convert df_info to a readable dataframe instead of list\n",
    "\n",
    "        \"\"\"\n",
    "        Row per column from the 'output' list\n",
    "        Columns: start-index, stop-index, NaN streak\n",
    "        \"\"\"\n",
    "\n",
    "        df_info = pd.DataFrame(columns=['Column name', 'Start index', 'Stop index', 'Amount of NaNs'])\n",
    "        length = len(output)\n",
    "        column_names = []\n",
    "        starts = []\n",
    "        stops = []\n",
    "        amounts = []\n",
    "\n",
    "        for column in range(length):\n",
    "            #print('At iteration %s of %s' % (column, length))\n",
    "            for i in range(len(output[column])):\n",
    "                column_names.append(df.columns[column])\n",
    "                starts.append(output[column][i][0])\n",
    "                stops.append(output[column][i][1])\n",
    "                amounts.append(output[column][i][2])\n",
    "\n",
    "        #print('Appending NaN info to df')\n",
    "        # Convert list to pd series\n",
    "        column_names = pd.Series(column_names)\n",
    "        starts = pd.Series(starts)\n",
    "        stops = pd.Series(stops)\n",
    "        amounts = pd.Series(amounts)\n",
    "        # Append pd series to a column\n",
    "        df_info['Column name'] = column_names.values\n",
    "        df_info['Start index'] = starts.values\n",
    "        df_info['Stop index'] = stops.values\n",
    "        df_info['Amount of NaNs'] = amounts.values\n",
    "\n",
    "        percentage = (df_info['Amount of NaNs'] / len(df)) * 100\n",
    "        df_info.drop(df_info[percentage < threshold_percentage].index, inplace=True)\n",
    "        return df_info\n",
    "\n",
    "    df_info = list_to_df(output)\n",
    "    \n",
    "    return df_info\n",
    "\n",
    "\n",
    "def save_df_unprocessed(df, dwelling_id):\n",
    "    \"\"\"\n",
    "    Save unprocessed dataframe.\n",
    "    :param df: Pandas DataFrame\n",
    "    :param dwelling_id: String\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    dir = 'F://datc//opschaler//combined_gas_smart_weather_dfs//unprocessed//'\n",
    "    df.to_csv(dir + dwelling_id + '.csv', sep='\\t', index=True)\n",
    "    print('Saved unprocessed df: %s' % dwelling_id)\n",
    "    return\n",
    "\n",
    "\n",
    "@delayed\n",
    "def drop_nan_streaks_above_threshold(df, df_nan_table, thresholds):\n",
    "    \"\"\"\n",
    "    Drops NaN streaks from the df when they are larger then the threshold value.\n",
    "    This function also inputs df_nan_table because it already has been made in the smart_gas_nan_checker.\n",
    "    :param df: Pandas DataDrame to process NaNs off\n",
    "    :param df_nan_table: NaN info Pandas DataFrame of the input df\n",
    "    :param thresholds: Dictionary {'column_name':column_threshold}, column_threshold has to be an integer.\n",
    "    :return: Pandas DataFrame\n",
    "    \"\"\"\n",
    "    df_nan_table = df_nan_table.compute()\n",
    "\n",
    "    # Check for NaN streaks > threshold and drop them from the df\n",
    "    length = len(df_nan_table['Amount of NaNs'])\n",
    "    #print('df_nan_table length: %s' % length)\n",
    "\n",
    "    indices_to_drop = []\n",
    "    for i, amount in enumerate(df_nan_table['Amount of NaNs']):\n",
    "        selected_column = df_nan_table['Column name'][i]\n",
    "        try:\n",
    "            if amount > thresholds[selected_column]:\n",
    "                start_index = (df_nan_table['Start index'][i])\n",
    "                stop_index = (df_nan_table['Stop index'][i])\n",
    "                indices = df[start_index:stop_index].index\n",
    "                #print('Enumeration %s of %s | From \\t %s \\t to \\t %s | column %s | NaN streak length: %s'\n",
    "                #      % (i, length, start_index, stop_index, selected_column, (len(indices))))\n",
    "                try:\n",
    "                    indices_to_drop += indices\n",
    "                except:\n",
    "                    print('Could not add indices to indices_to_drop list')\n",
    "            else:\n",
    "                #print('amount < threshold')\n",
    "                pass\n",
    "        except:\n",
    "            #print('No threshold detected for %s' % selected_column)\n",
    "            pass\n",
    "\n",
    "    #print('Dropping NaN streaks > threshold')\n",
    "    l1 = len(df)\n",
    "    df = df.drop(indices_to_drop)\n",
    "    l2 = len(df)\n",
    "    #print('Removed %s rows' % (l1-l2))\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_df_processed(df, dwelling_id):\n",
    "    \"\"\"\n",
    "    Save interpolated dataframe.\n",
    "    :param df: Pandas DataFrame\n",
    "    :param dwelling_id: String\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    dir = 'F://datc//opschaler//combined_gas_smart_weather_dfs//processed//'\n",
    "    df.to_csv(dir + dwelling_id + '.csv', sep='\\t', index=True)\n",
    "    print('Saved processed df: %s' % dwelling_id)\n",
    "    return\n",
    "\n",
    "def save_nan_table(nan_table, dwelling_id):\n",
    "    \"\"\"\n",
    "    Save nan table\n",
    "    :param df: Pandas DataFrame\n",
    "    :param dwelling_id: String\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    dir = 'F://datc//opschaler//nan_information//'\n",
    "    nan_table.to_csv(dir + dwelling_id + '.csv', sep='\\t', index=True)\n",
    "    print('Saved nan table from: %s' % dwelling_id)\n",
    "    return\n",
    "\n",
    "def save_nan_fig(fig, dwelling_id):\n",
    "    \"\"\"\n",
    "    Save nan fig\n",
    "    :param df: Pandas DataFrame\n",
    "    :param dwelling_id: String\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    dir = 'F://datc//opschaler//nan_information//figures//'\n",
    "    #fig.tight_layout()\n",
    "    #fig.tight_layout\n",
    "    fig.savefig(dir + dwelling_id + '.png', dpi=300)\n",
    "    #print('Saved nan fig from: %s' % dwelling_id)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "client.restart()\n",
    "\n",
    "weather = read_weather_data()\n",
    "weather_rs = weather.resample('10min').mean()\n",
    "\n",
    "file_paths, dwelling_ids = smartmeter_data()\n",
    "\n",
    "#file_paths = file_paths[:5]\n",
    "#dwelling_ids = dwelling_ids[:5]\n",
    "\n",
    "dfs_hour = []\n",
    "dfs_10s = []\n",
    "dfs_nan_table_10s = []\n",
    "dfs_nan_table_hour = []\n",
    "\n",
    "nan_figs_10s = []\n",
    "nan_figs_hour = []\n",
    "\n",
    "dfs_10s_partly_processed = []\n",
    "dfs_hour_partly_processed = []\n",
    "\n",
    "smarts = []\n",
    "gass = []\n",
    "\n",
    "\n",
    "for i, path in enumerate(file_paths):\n",
    "    dwelling_id = dwelling_ids[i]\n",
    "    \n",
    "    smart, gas = clean_prepare_smart_gas(path, dwelling_id)\n",
    "    \n",
    "    # client.persist: Start computing these variables and keep them in memory\n",
    "    smart = smart.persist()\n",
    "    gas = gas.persist()\n",
    "\n",
    "    smart, gas = resample_dfs(smart, gas)\n",
    "    \n",
    "    smart = smart.persist()\n",
    "    gas = gas.persist()\n",
    "    \n",
    "    df_hour = create_hour_df(smart, gas, weather, dwelling_id)\n",
    "    df_10s = create_10s_df(smart, gas, weather, dwelling_id)\n",
    "    \n",
    "    df_hour = df_hour.persist()\n",
    "    df_10s = df_10s.persist()\n",
    "    \n",
    "    #Slow, plus low cpu usage...\n",
    "    #nan_fig_10s = plot_nans(df_10s, dwelling_id+' 10s sample rate', '1T')\n",
    "    #nan_fig_hour = plot_nans(df_hour, dwelling_id+' one hour sample rate', 'H')\n",
    "    \n",
    "    #df_nan_table_10s = df_nan_checker(df_10s, 0)\n",
    "    #df_nan_table_hour = df_nan_checker(df_hour, 0)\n",
    "    \n",
    "    #df_nan_table_10s = df_nan_table_10s.persist()\n",
    "    #df_nan_table_hour = df_nan_table_hour.persist()\n",
    "    \n",
    "    #thresholds_10s = {'eMeter': 6, 'ePower': 6, 'gasMeter': 72, 'T': 36, 'Q': 18}\n",
    "    #df_10s_partly_processed = drop_nan_streaks_above_threshold(df_10s, df_nan_table_10s, thresholds_10s)\n",
    "    #df_10s_partly_processed = df_10s_partly_processed.persist()\n",
    "    \n",
    "    #thresholds_hour = {'gasMeter': 4, 'T': 4, 'Q': 4}\n",
    "    #df_hour_partly_processed = drop_nan_streaks_above_threshold(df_hour, df_nan_table_hour, thresholds_hour)\n",
    "    #df_hour_partly_processed = df_hour_partly_processed.persist()\n",
    "    \n",
    "    dfs_hour.append(df_hour)\n",
    "    dfs_10s.append(df_10s)\n",
    "    #dfs_nan_table_10s.append(df_nan_table_10s)\n",
    "    #dfs_nan_table_hour.append(df_nan_table_hour)\n",
    "    \n",
    "    #nan_figs_10s.append(nan_fig_10s)\n",
    "    #nan_figs_hour.append(nan_fig_hour)\n",
    "    \n",
    "    #dfs_10s_partly_processed.append(df_10s_partly_processed)\n",
    "    #dfs_hour_partly_processed.append(df_hour_partly_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_10s_results = []\n",
    "dfs_hour_results = []\n",
    "\n",
    "dfs_nan_table_10s_results = []\n",
    "dfs_nan_table_hour_results = []\n",
    "\n",
    "dfs_10s_partly_processed_results = []\n",
    "dfs_hour_partly_processed_results = []\n",
    "\n",
    "nan_figs_10s_results = []\n",
    "nan_figs_hour_results = []\n",
    "\n",
    "for i in range(len(dfs_10s)):\n",
    "    dfs_10s_results.append(client.compute(dfs_10s[i].compute()))\n",
    "    dfs_hour_results.append(client.compute(dfs_hour[i].compute()))\n",
    "    \n",
    "    #dfs_nan_table_10s_results.append(client.compute(dfs_nan_table_10s[i].compute()))\n",
    "    #dfs_nan_table_hour_results.append(client.compute(dfs_nan_table_hour[i].compute()))\n",
    "    \n",
    "    #dfs_10s_partly_processed_results.append(client.compute(dfs_10s_partly_processed[i].compute()))\n",
    "    #dfs_hour_partly_processed_results.append(client.compute(dfs_hour_partly_processed[i].compute()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished saving 0\n",
      "Finished saving 1\n",
      "Saved unprocessed df: P01S01W0001_10s\n",
      "Finished saving 0\n",
      "Saved unprocessed df: P01S01W0000_hour\n",
      "Finished saving 1\n",
      "Saved unprocessed df: P01S01W0001_hour\n",
      "Saved unprocessed df: P01S01W0000_10s\n"
     ]
    }
   ],
   "source": [
    "# Save unprocessed dfs\n",
    "\n",
    "zz = []\n",
    "for i in range(len(dfs_10s)):\n",
    "    dwelling_id = dwelling_ids[i]\n",
    "    df = dfs_10s[i].compute()\n",
    "    z = client.submit(save_df_unprocessed, df, dwelling_id+'_10s')\n",
    "    zz.append(z) # This makes it run in parallel?\n",
    "    print('Finished saving %s' % i)\n",
    "    \n",
    "zz = []\n",
    "for i in range(len(dfs_hour)):\n",
    "    dwelling_id = dwelling_ids[i]\n",
    "    df = dfs_hour[i].compute()\n",
    "    z = client.submit(save_df_unprocessed, df, dwelling_id+'_hour')\n",
    "    zz.append(z) # This makes it run in parallel?\n",
    "    print('Finished saving %s' % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished saving 0\n",
      "Finished saving 1\n",
      "Saved processed df: P01S01W0001_10s\n",
      "Finished saving 0\n",
      "Saved processed df: P01S01W0000_hour\n",
      "Saved processed df: P01S01W0000_10s\n",
      "Finished saving 1\n",
      "Saved processed df: P01S01W0001_hour\n"
     ]
    }
   ],
   "source": [
    "# Save processed dfs\n",
    "\n",
    "zz = []\n",
    "for i in range(len(dfs_10s_partly_processed)):\n",
    "    dwelling_id = dwelling_ids[i]\n",
    "    df = dfs_10s_partly_processed[i].compute()\n",
    "    z = client.submit(save_df_processed, df, dwelling_id+'_10s')\n",
    "    zz.append(z) # This makes it run in parallel?\n",
    "    print('Finished saving %s' % i)\n",
    "\n",
    "zz = []\n",
    "for i in range(len(dfs_hour_partly_processed)):\n",
    "    dwelling_id = dwelling_ids[i]\n",
    "    df = dfs_hour_partly_processed[i].compute()\n",
    "    z = client.submit(save_df_processed, df, dwelling_id+'_hour')\n",
    "    zz.append(z) # This makes it run in parallel?\n",
    "    print('Finished saving %s' % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished saving 0\n",
      "Finished saving 1\n",
      "Saved nan table from: P01S01W0000_10s\n",
      "Saved nan table from: P01S01W0001_10s\n",
      "Finished saving 0\n",
      "Saved nan table from: P01S01W0000_hour\n",
      "Finished saving 1\n",
      "Saved nan table from: P01S01W0001_hour\n"
     ]
    }
   ],
   "source": [
    "# Save nan tables\n",
    "\n",
    "zz = []\n",
    "for i in range(len(dfs_10s_partly_processed)):\n",
    "    dwelling_id = dwelling_ids[i]\n",
    "    df = dfs_nan_table_10s_results[i].result()\n",
    "    z = client.submit(save_nan_table, df, dwelling_id+'_10s')\n",
    "    zz.append(z) # This makes it run in parallel?\n",
    "    print('Finished saving %s' % i)\n",
    "\n",
    "zz = []\n",
    "for i in range(len(dfs_hour_partly_processed)):\n",
    "    dwelling_id = dwelling_ids[i]\n",
    "    df = dfs_nan_table_hour_results[i].result()\n",
    "    z = client.submit(save_nan_table, df, dwelling_id+'_hour')\n",
    "    zz.append(z) # This makes it run in parallel?\n",
    "    print('Finished saving %s' % i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create & save NaN figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                           | 0/56 [00:00<?, ?it/s]\n",
      "\n",
      "  2%|█▍                                                                                 | 1/56 [00:00<00:20,  2.70it/s]\n",
      "\n",
      "  4%|██▉                                                                                | 2/56 [00:00<00:18,  2.87it/s]\n",
      "\n",
      "  5%|████▍                                                                              | 3/56 [00:01<00:20,  2.62it/s]\n",
      "\n",
      "  7%|█████▉                                                                             | 4/56 [00:01<00:25,  2.07it/s]\n",
      "\n",
      "  9%|███████▍                                                                           | 5/56 [00:02<00:24,  2.07it/s]\n",
      "\n",
      " 11%|████████▉                                                                          | 6/56 [00:03<00:26,  1.87it/s]\n",
      "\n",
      " 12%|██████████▍                                                                        | 7/56 [00:04<00:28,  1.72it/s]\n",
      "\n",
      " 14%|███████████▊                                                                       | 8/56 [00:04<00:27,  1.77it/s]\n",
      "\n",
      " 16%|█████████████▎                                                                     | 9/56 [00:05<00:28,  1.68it/s]\n",
      "\n",
      " 18%|██████████████▋                                                                   | 10/56 [00:07<00:33,  1.39it/s]\n",
      "\n",
      " 20%|████████████████                                                                  | 11/56 [00:08<00:32,  1.37it/s]c:\\program files\\python36\\lib\\site-packages\\tqdm\\_monitor.py:89: TqdmSynchronisationWarning: Set changed size during iteration (see https://github.com/tqdm/tqdm/issues/481)\n",
      "  TqdmSynchronisationWarning)\n",
      "\n",
      "\n",
      " 21%|█████████████████▌                                                                | 12/56 [00:08<00:32,  1.35it/s]\n",
      "\n",
      " 23%|███████████████████                                                               | 13/56 [00:09<00:32,  1.34it/s]\n",
      "\n",
      " 25%|████████████████████▌                                                             | 14/56 [00:10<00:31,  1.33it/s]\n",
      "\n",
      " 27%|█████████████████████▉                                                            | 15/56 [00:11<00:30,  1.33it/s]\n",
      "\n",
      " 29%|███████████████████████▍                                                          | 16/56 [00:12<00:30,  1.32it/s]\n",
      "\n",
      " 30%|████████████████████████▉                                                         | 17/56 [00:12<00:29,  1.31it/s]\n",
      "\n",
      " 32%|██████████████████████████▎                                                       | 18/56 [00:13<00:29,  1.31it/s]\n",
      "\n",
      " 34%|███████████████████████████▊                                                      | 19/56 [00:14<00:28,  1.28it/s]\n",
      "\n",
      " 36%|█████████████████████████████▎                                                    | 20/56 [00:15<00:27,  1.30it/s]\n",
      "\n",
      " 38%|██████████████████████████████▊                                                   | 21/56 [00:16<00:26,  1.30it/s]\n",
      "\n",
      " 39%|████████████████████████████████▏                                                 | 22/56 [00:17<00:26,  1.29it/s]\n",
      "\n",
      " 41%|█████████████████████████████████▋                                                | 23/56 [00:18<00:26,  1.24it/s]\n",
      "\n",
      " 43%|███████████████████████████████████▏                                              | 24/56 [00:19<00:25,  1.24it/s]\n",
      "\n",
      " 45%|████████████████████████████████████▌                                             | 25/56 [00:20<00:25,  1.24it/s]\n",
      "\n",
      " 46%|██████████████████████████████████████                                            | 26/56 [00:21<00:24,  1.24it/s]\n",
      "\n",
      " 48%|███████████████████████████████████████▌                                          | 27/56 [00:22<00:24,  1.20it/s]\n",
      "\n",
      " 50%|█████████████████████████████████████████                                         | 28/56 [00:23<00:23,  1.18it/s]\n",
      "\n",
      " 52%|██████████████████████████████████████████▍                                       | 29/56 [00:24<00:22,  1.18it/s]\n",
      "\n",
      " 54%|███████████████████████████████████████████▉                                      | 30/56 [00:25<00:22,  1.17it/s]\n",
      "\n",
      " 55%|█████████████████████████████████████████████▍                                    | 31/56 [00:26<00:21,  1.17it/s]\n",
      "\n",
      " 57%|██████████████████████████████████████████████▊                                   | 32/56 [00:27<00:20,  1.15it/s]\n",
      "\n",
      " 59%|████████████████████████████████████████████████▎                                 | 33/56 [00:28<00:19,  1.15it/s]\n",
      "\n",
      " 61%|█████████████████████████████████████████████████▊                                | 34/56 [00:29<00:19,  1.16it/s]\n",
      "\n",
      " 62%|███████████████████████████████████████████████████▎                              | 35/56 [00:30<00:18,  1.16it/s]\n",
      "\n",
      " 64%|████████████████████████████████████████████████████▋                             | 36/56 [00:31<00:17,  1.15it/s]\n",
      "\n",
      " 66%|██████████████████████████████████████████████████████▏                           | 37/56 [00:31<00:16,  1.16it/s]\n",
      "\n",
      " 68%|███████████████████████████████████████████████████████▋                          | 38/56 [00:32<00:15,  1.16it/s]\n",
      "\n",
      " 70%|█████████████████████████████████████████████████████████                         | 39/56 [00:33<00:14,  1.17it/s]\n",
      "\n",
      " 71%|██████████████████████████████████████████████████████████▌                       | 40/56 [00:33<00:13,  1.18it/s]\n",
      "\n",
      " 73%|████████████████████████████████████████████████████████████                      | 41/56 [00:34<00:12,  1.18it/s]\n",
      "\n",
      " 75%|█████████████████████████████████████████████████████████████▌                    | 42/56 [00:35<00:11,  1.18it/s]\n",
      "\n",
      " 77%|██████████████████████████████████████████████████████████████▉                   | 43/56 [00:37<00:11,  1.16it/s]\n",
      "\n",
      " 79%|████████████████████████████████████████████████████████████████▍                 | 44/56 [00:37<00:10,  1.16it/s]\n",
      "\n",
      " 80%|█████████████████████████████████████████████████████████████████▉                | 45/56 [00:38<00:09,  1.17it/s]\n",
      "\n",
      " 82%|███████████████████████████████████████████████████████████████████▎              | 46/56 [00:39<00:08,  1.16it/s]\n",
      "\n",
      " 84%|████████████████████████████████████████████████████████████████████▊             | 47/56 [00:40<00:07,  1.16it/s]\n",
      "\n",
      " 86%|██████████████████████████████████████████████████████████████████████▎           | 48/56 [00:41<00:06,  1.15it/s]\n",
      "\n",
      " 88%|███████████████████████████████████████████████████████████████████████▊          | 49/56 [00:42<00:06,  1.14it/s]\n",
      "\n",
      " 89%|█████████████████████████████████████████████████████████████████████████▏        | 50/56 [00:43<00:05,  1.14it/s]\n",
      "\n",
      " 91%|██████████████████████████████████████████████████████████████████████████▋       | 51/56 [00:45<00:04,  1.13it/s]\n",
      "\n",
      " 93%|████████████████████████████████████████████████████████████████████████████▏     | 52/56 [00:46<00:03,  1.12it/s]\n",
      "\n",
      " 95%|█████████████████████████████████████████████████████████████████████████████▌    | 53/56 [00:47<00:02,  1.11it/s]\n",
      "\n",
      " 96%|███████████████████████████████████████████████████████████████████████████████   | 54/56 [00:48<00:01,  1.10it/s]\n",
      "\n",
      " 98%|████████████████████████████████████████████████████████████████████████████████▌ | 55/56 [00:49<00:00,  1.10it/s]\n",
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 56/56 [00:51<00:00,  1.09it/s]\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                           | 0/56 [00:00<?, ?it/s]\n",
      "\n",
      "  2%|█▍                                                                                 | 1/56 [00:01<00:56,  1.03s/it]\n",
      "\n",
      "  4%|██▉                                                                                | 2/56 [00:01<00:37,  1.45it/s]\n",
      "\n",
      "  5%|████▍                                                                              | 3/56 [00:09<02:50,  3.21s/it]\n",
      "\n",
      "  7%|█████▉                                                                             | 4/56 [00:39<08:38,  9.98s/it]\n",
      "\n",
      "  9%|███████▍                                                                           | 5/56 [00:49<08:21,  9.83s/it]\n",
      "\n",
      " 11%|████████▉                                                                          | 6/56 [01:26<11:58, 14.36s/it]"
     ]
    }
   ],
   "source": [
    "# Save nan figures, non-daks way. Using dask gives errors\n",
    "\n",
    "for i in tqdm(range(len(dfs_10s_results))):\n",
    "    plt.close()\n",
    "    dwelling_id = dwelling_ids[i]\n",
    "    df_hour = dfs_hour_results[i]\n",
    "    fig = plot_nans(df_hour, dwelling_id+' one hour sample rate', 'H')\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    dir = 'F://datc//opschaler//nan_information//figures//'\n",
    "    fig.savefig(dir + dwelling_id + '_hour.png', dpi=300)\n",
    "\n",
    "    \n",
    "for i in tqdm(range(len(dfs_10s_results))):\n",
    "    plt.close()\n",
    "    dwelling_id = dwelling_ids[i]\n",
    "    df_10s = dfs_10s_results[i]\n",
    "    fig = plot_nans(df_10s, dwelling_id+' 10s sample rate', '1T')\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    dir = 'F://datc//opschaler//nan_information//figures//'\n",
    "    fig.savefig(dir + dwelling_id+ '_10s.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-fdea80f44bcd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfigs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "figs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved nan fig from: P01S01W0000\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save nan figures\n",
    "\n",
    "zz = []\n",
    "for i in range(len(nan_figs_10s_results)):\n",
    "    dwelling_id = dwelling_ids[i]\n",
    "    fig = nan_figs_10s_results[i].result()\n",
    "    z = client.submit(save_nan_fig, fig, dwelling_id+'_10s')\n",
    "    zz.append(z) # This makes it run in parallel?\n",
    "    print('Finished saving %s' % i)\n",
    "\n",
    "zz = []\n",
    "for i in range(len(nan_figs_hour_results)):\n",
    "    dwelling_id = dwelling_ids[i]\n",
    "    fig = nan_figs_hour_results[i].result()\n",
    "    z = client.submit(save_nan_fig, fig, dwelling_id+'_hour')\n",
    "    zz.append(z) # This makes it run in parallel?\n",
    "    print('Finished saving %s' % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
