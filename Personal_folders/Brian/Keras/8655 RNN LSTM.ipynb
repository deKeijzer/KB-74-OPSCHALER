{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import dask.dataframe as dd\n",
    "\n",
    "mpl.style.use('default')\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow backend initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/jupyterhub/anaconda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Setup multi GPU usage\n",
    "\n",
    "Example usage:\n",
    "model = Sequential()\n",
    "...\n",
    "multi_model = multi_gpu_model(model, gpus=num_gpu)\n",
    "multi_model.fit()\n",
    "\n",
    "About memory usage:\n",
    "https://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# IMPORTANT: Tells tf to not occupy a specific amount of memory\n",
    "from keras.backend.tensorflow_backend import set_session  \n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU  \n",
    "sess = tf.Session(config=config)  \n",
    "set_session(sess)  # set this TensorFlow session as the default session for Keras.\n",
    "\n",
    "\n",
    "# getting the number of GPUs \n",
    "def get_available_gpus():\n",
    "   local_device_protos = device_lib.list_local_devices()\n",
    "   return [x.name for x in local_device_protos if x.device_type    == 'GPU']\n",
    "num_gpu = len(get_available_gpus())\n",
    "print('Amount of GPUs available: %s' % num_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corr_matrix(df, dwelling_id, annot):\n",
    "    \"\"\"\n",
    "    Pearson correlation coefficient matrix. \n",
    "    The Pearson correlation coefficient is a measure of the linear correlation between two variables.\n",
    "    \"\"\"\n",
    "    plt.clf()\n",
    "    \n",
    "    corr = df.corr()\n",
    "    mask = np.zeros_like(corr)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    if annot:\n",
    "        fig, ax = plt.subplots(figsize=(25,25))\n",
    "    else:\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "    fig = sns.heatmap(corr, mask=mask, square=False, cmap='RdYlGn', annot=annot, ax=ax, \n",
    "                cbar_kws={'label':'Pearson correlation coefficient [-]'})\n",
    "\n",
    "    fig.set_title('Correlation matrix of dwelling ID: '+dwelling_id)\n",
    "    fig.tick_params(axis='x', rotation=90)\n",
    "    fig.tick_params(axis='y', rotation=0)\n",
    "\n",
    "    fig = fig.get_figure()\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    \n",
    "    print('Saving heatmap')\n",
    "    #fig.savefig('//datc//opschaler//EDA//Pearson_corr//' + dwelling_id + '.png', dpi=300)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def reduce_memory(df):\n",
    "    \"\"\"\n",
    "    Reduces memory footprint of the input dataframe.\n",
    "    Changes float64 columns to float32 dtype.\n",
    "    \"\"\"\n",
    "    columns = df.columns\n",
    "    memory_before = df.memory_usage(deep=False).sum() / 2**30 # convert bytes to GB\n",
    "\n",
    "    for column in tqdm(columns):\n",
    "        if df[column].dtype == 'float64':\n",
    "            df[column] = df[column].astype('float32')\n",
    "        \n",
    "    memory_after = df.memory_usage(deep=False).sum() / 2**30 # convert bytes to GB\n",
    "    print('Memory uasge reduced from %.3f GB to %.3f GB' % (memory_before, memory_after))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def resample_df(df, sample_rate, combine_all_dwellings=False):\n",
    "    \"\"\"\n",
    "    Resampled a (un)processed dataframe to the specified sample_rate.\n",
    "    Input is a (un)processed df.\n",
    "    Input df may also be multiple dwelling dfs combined.\n",
    "    Sample rate must be a string. \n",
    "    For example '1H', '1D', '60s'.\n",
    "    \n",
    "    Combine all dwellings: resamples the df and ignores the fact that there are unique dwellings.\n",
    "    \n",
    "    TODO: add std to ePower, gasPower when combine_all_dwellings=False\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    def resample_dwelling(df, sample_rate, dwelling_id):\n",
    "        df = df.resample(sample_rate).mean() # resample to rest by mean\n",
    "        df['dwelling'] = dwelling_id\n",
    "        return df\n",
    "        \n",
    "                      \n",
    "    resampled_dwellings = []\n",
    "    \n",
    "    if combine_all_dwellings: # Ignore dwelling_ids\n",
    "        df = df.drop(['eMeter', 'eMeterReturn', 'eMeterLow', 'eMeterLowReturn', 'gasMeter'], axis=1) # Drop columns because they are meaningless when ignoring dwelling ids\n",
    "        resampled_df = resample_dwelling(df, sample_rate, 'All dwellings')\n",
    "        resampled_dwellings.append(resampled_df)\n",
    "    else:\n",
    "        dwellings = df['dwelling'].unique() # Get dwelling ids\n",
    "        for dwelling_id in tqdm(dwellings):\n",
    "            dwelling_df = df[df['dwelling'] == dwelling_id] # Get the data from only that dwelling_id\n",
    "            resampled_dwelling = resample_dwelling(dwelling_df, sample_rate, dwelling_id)\n",
    "            resampled_dwellings.append(resampled_dwelling)\n",
    "    \n",
    "    resampled_df = pd.concat(resampled_dwellings)\n",
    "    \n",
    "    return resampled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing dataframe for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_csv('//datc//opschaler//combined_gas_smart_weather_dfs//processed//P01S01W8655_hour.csv', delimiter='\\t', parse_dates=['datetime'])\n",
    "df = df.set_index(['datetime'])\n",
    "df = df.dropna()\n",
    "\n",
    "# Get an hour dataframe\n",
    "df = resample_df(df, 'D') # For some reason this makes df equal h, so it changes the df variable being read in at the start...\n",
    "#df['year'] = df.index.year\n",
    "#df['month'] = df.index.month\n",
    "#df['day'] = df.index.day\n",
    "#df['hour'] = df.index.hour #create column containing the hour\n",
    "#df['dayofweek'] = df.index.dayofweek\n",
    "#df['season'] = (df.index.month%12 + 3)//3 # Calculates the season (categorical)\n",
    "\n",
    "df = df.iloc[-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a proper hourly dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = create_corr_matrix(df, '', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select data we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df\n",
    "data = data.drop(['eMeter', 'eMeterReturn', 'eMeterLow', 'eMeterLowReturn', 'gasMeter', 'dwelling'], axis=1) # Not needed\n",
    "data = data.drop(['WW', 'VV', 'P', 'DR', 'SQ', 'TD', 'T10', 'FX'], axis=1) # Drop weather columns which contain correlated information, keep only one type\n",
    "#sns.heatmap(data.corr(), annot=True)\n",
    "\n",
    "data = data.drop(['ePower', 'ePowerReturn'], axis=1) # Drop if want to predict gasPower\n",
    "\n",
    "# Drop columns with that have a |corr| > 0.1 with T\n",
    "data = data.drop(['U', 'N', 'Q', 'DD'], axis=1)\n",
    "\n",
    "\n",
    "#data = data[data['T'] < 0] #filter data based on condition\n",
    "#data = data.reset_index()\n",
    "magnitude = 1\n",
    "data['gasPower'] = data['gasPower']*10**magnitude\n",
    "data = data.dropna()\n",
    "\n",
    "sns.heatmap(data.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data['T'], data['gasPower'], '.', color='r', label='Original data')\n",
    "plt.xlabel('Temperature [Â°C]')\n",
    "plt.ylabel(r'gasPower $\\cdot$ 10$^{-%s}$ [m$^3$/h]' % magnitude)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.pairplot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_timeseries_history(df, columns_to_lookback, n_lookback, dropnan=True):\n",
    "    \"\"\"\n",
    "    df, pandas dataframe\n",
    "    columns_to_lookback, the columns to gather historical data from\n",
    "    n_lookback, amount of samples to look back for. \n",
    "    \n",
    "    Example:\n",
    "    df = dwelling_df['FF']\n",
    "    columns_to_lookback = df.columns\n",
    "    n_lookback = 3\n",
    "    \n",
    "    \n",
    "    Output columns:\n",
    "    FF, FF (t-1), FF (t-2), FF (t-3)\n",
    "    \n",
    "    Where FF (t-3) contains the FF value of 3 samples (indices) back.\n",
    "    \"\"\"\n",
    "    n_lookback += 1 # +1 because iteration starts at 0.\n",
    "    df = df.copy()\n",
    "    \n",
    "    for column in columns_to_lookback:\n",
    "        if column == 'datetime':\n",
    "            pass\n",
    "        else:\n",
    "            for dt in range(n_lookback):\n",
    "                if dt == 0:\n",
    "                    pass\n",
    "                else: \n",
    "                    df[column+'(t-%s)' % dt ] = df[column].shift(dt)\n",
    "    if dropnan:\n",
    "        df = df.dropna()\n",
    "        \n",
    "    return df\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    columns = data.columns\n",
    "    \n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    dff = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(dff.shift(i))\n",
    "        names += [('%s(t-%d)' % (columns[j], i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(dff.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('%s(t)' % (columns[j])) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('%s(t+%d)' % (columns[j], i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "# Create history data\n",
    "#data = create_timeseries_history(data, columns_to_lookback=data.columns, n_lookback=1, dropnan=True)\n",
    "data = series_to_supervised(data, 1, 7) \n",
    "print('Data columns: %s' % list(data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['gasPower(t)'], axis=1)\n",
    "print('X columns: %s' % list(X.columns))\n",
    "\n",
    "#sns.heatmap(X.corr(), annot=True)\n",
    "\n",
    "y = data[['gasPower(t)']]\n",
    "\n",
    "# Get it in the right shape\n",
    "X = np.array(X).reshape(-1, len(X.columns))\n",
    "y = np.array(y).reshape(-1, len(y.columns))\n",
    "\n",
    "print('X shape: ', X.shape)\n",
    "print('y shape: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape and scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_reshape(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Input: X and y as pandas dataframes.\n",
    "    Scales and reshapes X and y. \n",
    "    Output: X and y as numpy ndarrays.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Scale the data\n",
    "    scalerX = StandardScaler(with_mean=True, with_std=True).fit(X_train)\n",
    "    #scalery = StandardScaler(with_mean=True, with_std=True).fit(y_train)\n",
    "\n",
    "    # Normalize X arrrays\n",
    "    X_train = scalerX.transform(X_train)\n",
    "    X_test = scalerX.transform(X_test)\n",
    "\n",
    "    # Get test data in the correct shape and format\n",
    "    y_train = np.array(y_train).reshape(-1,1) #reshape if y_train is not scaled, transforming should not be needed\n",
    "    y_test = np.array(y_test).reshape(-1,1)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def to_LSTM_shape(array):\n",
    "    \"\"\"\n",
    "    Reshapes a 2D array to 3D for LSTM usage (samples, timesteps, feautures). \n",
    "    For example, reshapes (104, 8) to (104, 1, 8)#\n",
    "    \n",
    "    \"\"\"\n",
    "    if len(array.shape) == 3:\n",
    "        return array\n",
    "    \n",
    "    array = array.reshape(array.shape[0], 1, array.shape[1])\n",
    "\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict gasPower with most other variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Dense, Conv1D, MaxPool2D, Flatten, Dropout\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.optimizers import Adam, SGD, Nadam\n",
    "from time import time\n",
    "from livelossplot import PlotLossesKeras\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "\n",
    "def abs_percentage_error(y_true, y_pred):\n",
    "    import keras.backend as K\n",
    "    \"\"\"\n",
    "    Returns the absolute value of the difference between y_true and y_pred (in percentage).\n",
    "    For examples on losses see:\n",
    "    https://github.com/keras-team/keras/blob/master/keras/losses.py\n",
    "    \"\"\"\n",
    "    return (K.abs(y_true - y_pred) / K.abs(y_pred)) * 100\n",
    "\n",
    "\n",
    "def make_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    length = X_train.shape[2]\n",
    "    \n",
    "    model.add(LSTM(length, input_shape=(X_train.shape[1], X_train.shape[2]), kernel_initializer='TruncatedNormal', return_sequences=True))\n",
    "    model.add(LeakyReLU())\n",
    "    \n",
    "    for i in range(length*2):\n",
    "        model.add(LSTM(length, kernel_initializer='TruncatedNormal', return_sequences=True))\n",
    "        model.add(LeakyReLU())\n",
    "    \n",
    "    #model.add(Dropout(0.5))\n",
    "    \n",
    "    #model.add(Flatten())\n",
    "    \n",
    "    N = 256 #45\n",
    "    \n",
    "    #for i in range(4):\n",
    "    #    model.add(Dense(N-i*4, kernel_initializer='normal'))\n",
    "    #    model.add(Activation('relu'))\n",
    "    #    model.add(Dropout(0.5, seed=seed))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(1, kernel_initializer='TruncatedNormal'))\n",
    "    model.add(Activation('linear'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def compile_fit(model):\n",
    "    # Compile and fit\n",
    "    #model = multi_gpu_model(model, gpus=num_gpu)\n",
    "\n",
    "    lr = 0.01\n",
    "    epochs = 500\n",
    "    decay_rate = 0\n",
    "    \n",
    "    sgd = SGD(lr=lr, momentum=0.9, decay=decay_rate, nesterov=True) # sgd in general yields better results, but needs a lot of tweeking and is slower\n",
    "\n",
    "    # compiling the sequential model\n",
    "    #multi_model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.compile(loss='mse', metrics=['mse', abs_percentage_error], optimizer=sgd)\n",
    "\n",
    "    early_stopping_monitor = EarlyStopping(patience=1)\n",
    "\n",
    "    #tensorboard = TensorBoard(log_dir=\"/home/16011015/notebooks/logs/{}\".format(time()))\n",
    "\n",
    "    # training the model and saving metrics in history\n",
    "    model.fit(X_train, y_train, batch_size=int(len(X_train)), \n",
    "                    epochs=epochs, verbose=2, validation_data=(X_test, y_test), \n",
    "                    callbacks=[early_stopping_monitor, PlotLossesKeras()])\n",
    "    \n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile & fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "scores = []\n",
    "y_tests = []\n",
    "y_preds = []\n",
    "test_indices = []\n",
    "\n",
    "n = 2*6\n",
    "for train_index, test_index in TimeSeriesSplit(n_splits=n, max_train_size=None).split(X):\n",
    "    # Train on one month, try to predict 1 week ahead, n_splits=26, max_train_size=4*7\n",
    "    print(\"Train: %s \\t Test: %s\" % (train_index, test_index))\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index] # could manually assign a max size to this\n",
    "    \n",
    "    #Scale reshape\n",
    "    X_train, X_test, y_train, y_test = scale_reshape(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    X_train = to_LSTM_shape(X_train)\n",
    "    X_test = to_LSTM_shape(X_test)\n",
    "    \n",
    "    print('X shape: ', X_train.shape)\n",
    "    # Create the keras model\n",
    "    model = make_model()\n",
    "    \n",
    "    # Compile and fit the model\n",
    "    model = compile_fit(model)\n",
    "    \n",
    "    scores.append(model.evaluate(X_test, y_test))\n",
    "    y_tests.append(y_test)\n",
    "    y_preds.append(model.predict(X_test))\n",
    "    test_indices.append(test_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe containing the model scores\n",
    "df_scores = pd.DataFrame(scores, columns=['loss', 'mse', 'abs percentage error'])\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_t = np.vstack(y_tests) # List of ndarrays to one ndarray\n",
    "y_p = np.vstack(y_preds)\n",
    "\n",
    "# Get the x values for y predictions\n",
    "start = np.concatenate(test_indices)[0] # start y pred index\n",
    "stop = np.concatenate(test_indices)[-1] # stop y pred index\n",
    "x = data.index[start:stop+1] # series of dates from start to stop+1\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(x, y_t, '.-', color='r')\n",
    "plt.plot(x, y_p, '.-', color='b')\n",
    "\n",
    "plt.plot(data.index[:start], data['gasPower(t)'][:start], '.-', color='seagreen', label='Real values') # shows training data\n",
    "plt.plot(x, y_t, '.-', color='firebrick', label='Real values', alpha=1)\n",
    "plt.plot(x, y_p, '.-', color='royalblue', label='Predicted values', alpha=1)\n",
    "\n",
    "plt.xlabel('datetime [-]')\n",
    "plt.ylabel(r'gasPower $\\cdot$ 10$^{-%s}$ [m$^3$/h]' % magnitude)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "mse_mean = df_scores['mse'].mean()\n",
    "mse_std = df_scores['mse'].std()\n",
    "percentage_mean = df_scores['abs percentage error'].mean()\n",
    "percentage_std = df_scores['abs percentage error'].std()\n",
    "\n",
    "plt.title('Deep neural network regression results with gasPower as the target \\n \\\n",
    "          MSE$_{mean}$ = %.2f \\t MSE$_{std}$ = %.2f \\n \\\n",
    "          Error in percentage:    mean = %.2f [%%]    std = %.2f [%%]' % (mse_mean, mse_std, percentage_mean, percentage_std), fontsize = 14)\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('figures/8655 result.png', dpi=1000)\n",
    "print('FINISHED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('DDN last result.png', dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('DDN last result.png', dpi=1200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
