{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keijzer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4c601e77b01d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeijzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msetup_multi_gpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_corr_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_to_lstm_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'default'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keijzer'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Dense, Conv1D, MaxPool2D, Flatten, Dropout\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.optimizers import Adam, SGD, Nadam\n",
    "from time import time\n",
    "from livelossplot import PlotLossesKeras\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "import tensorflow as tf\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "from tensorflow.python.client import device_lib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keijzer import setup_multi_gpus, create_corr_matrix, reduce_memory, resample_df, df_to_lstm_format\n",
    "\n",
    "mpl.style.use('default')\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup multi GPU usage\n",
    "num_gpu = setup_multi_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('//datc//opschaler//combined_gas_smart_weather_dfs//processed/all_dwellings_combined_hour.csv', delimiter='\\t', parse_dates=['datetime'])\n",
    "df = df.set_index(['datetime'])\n",
    "df = df.dropna()\n",
    "\n",
    "# Get an hour dataframe\n",
    "df = resample_df(df, 'H', combine_all_dwellings=True)\n",
    "\n",
    "#df['year'] = df.index.year\n",
    "#df['month'] = df.index.month\n",
    "#df['day'] = df.index.day\n",
    "df['hour'] = df.index.hour #create column containing the hour\n",
    "df['dayofweek'] = df.index.dayofweek\n",
    "df['season'] = (df.index.month%12 + 3)//3 # Calculates the season (categorical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = create_corr_matrix(df, '', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select data to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df\n",
    "#data = data.drop(['eMeter', 'eMeterReturn', 'eMeterLow', 'eMeterLowReturn', 'gasMeter', 'dwelling'], axis=1) # Not needed\n",
    "data = data.drop(['dwelling'], axis=1) # Not needed\n",
    "data = data.drop(['WW', 'VV', 'P', 'DR', 'SQ', 'TD', 'T10', 'FX'], axis=1) # Drop weather columns which contain correlated information, keep only one type\n",
    "#sns.heatmap(data.corr(), annot=True)\n",
    "\n",
    "data = data.drop(['ePower', 'ePowerReturn'], axis=1) # Drop if want to predict gasPower\n",
    "\n",
    "# Drop columns with that have a |corr| > 0.1 with T\n",
    "data = data.drop(['U', 'N', 'Q', 'DD'], axis=1)\n",
    "\n",
    "\n",
    "#data = data[data['T'] < 0] #filter data based on condition\n",
    "#data = data.reset_index()\n",
    "magnitude = 1\n",
    "data['gasPower'] = data['gasPower']*10**magnitude\n",
    "data = data.dropna()\n",
    "\n",
    "fig = create_corr_matrix(data, '', True, size=(5,5))\n",
    "\n",
    "print('Len of data: ', len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "plt.plot(data.index, data['gasPower'], '.-', color='red', label='Original data', alpha=0.8)\n",
    "plt.xlabel('Datetime [-]', fontsize=20)\n",
    "plt.ylabel(r'gasPower $\\cdot$ 10$^{-%s}$ [m$^3$/h]' % (magnitude), fontsize=20)\n",
    "\n",
    "plt.xticks(fontsize=20, rotation=45)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "plt.title('Mean gas usage of 54 houses', fontsize=28)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('figures/available data.png', dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns_to_cat = ['year', 'month', 'day', 'hour', 'dayofweek', 'season']\n",
    "columns_to_cat = ['hour', 'dayofweek', 'season']\n",
    "data[columns_to_cat] = data[columns_to_cat].astype('category') # change datetypes to category\n",
    "\n",
    "data = pd.get_dummies(data, columns=columns_to_cat) # One hot encoding the categories\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Add a copy of gasPower column, so previous gasPower values are also in X_reshaped\n",
    "\"\"\"\n",
    "data['gasPower_copy'] = data['gasPower']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing, data to lstm format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 5 # D -> 5\n",
    "num_features = data.shape[1] - 1\n",
    "output_dim = 1\n",
    "test_size = 0.9\n",
    "\n",
    "X_train, y_train, X_test, y_test = df_to_lstm_format(df=data, test_size=test_size, look_back=look_back, target_column='gasPower', scale_X=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred):\n",
    "    import keras.backend as K\n",
    "    \"\"\"\n",
    "    Returns the mean absolute percentage error.\n",
    "    For examples on losses see:\n",
    "    https://github.com/keras-team/keras/blob/master/keras/losses.py\n",
    "    \"\"\"\n",
    "    return (K.abs(y_true - y_pred) / K.abs(y_pred)) * 100\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    import keras.backend as K\n",
    "    \"\"\"\n",
    "    Returns the Symmetric mean absolute percentage error.\n",
    "    For examples on losses see:\n",
    "    https://github.com/keras-team/keras/blob/master/keras/losses.py\n",
    "    \"\"\"\n",
    "    return (K.abs(y_pred - y_true) / ((K.abs(y_true) + K.abs(y_pred))))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to sequence GNU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from livelossplot import PlotLossesKeras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [35*2, 35*2]\n",
    "regulariser = None # Possible regulariser: keras.regularizers.l2(lambda_regulariser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an input sequence.\n",
    "encoder_inputs = keras.layers.Input(shape=(None, num_features))\n",
    "\n",
    "# Create a list of RNN Cells, these are then concatenated into a single layer\n",
    "# with the RNN layer.\n",
    "encoder_cells = []\n",
    "for hidden_neurons in layers:\n",
    "    encoder_cells.append(keras.layers.GRUCell(hidden_neurons,\n",
    "                                              kernel_regularizer=regulariser,\n",
    "                                              recurrent_regularizer=regulariser,\n",
    "                                              bias_regularizer=regulariser,\n",
    "                                              kernel_initializer='TruncatedNormal'))\n",
    "\n",
    "encoder = keras.layers.RNN(encoder_cells, return_state=True)\n",
    "\n",
    "encoder_outputs_and_states = encoder(encoder_inputs)\n",
    "\n",
    "# Discard encoder outputs and only keep the states.\n",
    "# The outputs are of no interest to us, the encoder's\n",
    "# job is to create a state describing the input sequence.\n",
    "encoder_states = encoder_outputs_and_states[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The decoder input will be set to zero (see random_sine function of the utils module).\n",
    "# Do not worry about the input size being 1, I will explain that in the next cell.\n",
    "decoder_inputs = keras.layers.Input(shape=(None, 1))\n",
    "\n",
    "decoder_cells = []\n",
    "for hidden_neurons in layers:\n",
    "    decoder_cells.append(keras.layers.GRUCell(hidden_neurons,\n",
    "                                              kernel_regularizer=regulariser,\n",
    "                                              recurrent_regularizer=regulariser,\n",
    "                                              bias_regularizer=regulariser,\n",
    "                                             kernel_initializer='TruncatedNormal'))\n",
    "\n",
    "decoder = keras.layers.RNN(decoder_cells, return_sequences=True, return_state=True)\n",
    "\n",
    "# Set the initial state of the decoder to be the ouput state of the encoder.\n",
    "# This is the fundamental part of the encoder-decoder.\n",
    "decoder_outputs_and_states = decoder(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "# Only select the output of the decoder (not the states)\n",
    "decoder_outputs = decoder_outputs_and_states[0]\n",
    "\n",
    "# Apply a dense layer with linear activation to set output to correct dimension\n",
    "# and scale (tanh is default activation for GRU in Keras, our output sine function can be larger then 1)\n",
    "decoder_dense = keras.layers.Dense(output_dim,\n",
    "                                   activation='linear',\n",
    "                                   kernel_regularizer=regulariser,\n",
    "                                   bias_regularizer=regulariser)\n",
    "\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model using the functional API provided by Keras.\n",
    "# The functional API is great, it gives an amazing amount of freedom in architecture of your NN.\n",
    "# A read worth your time: https://keras.io/getting-started/functional-api-guide/ \n",
    "model = keras.models.Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_outputs)\n",
    "\n",
    "sgd = SGD(lr=0.05, momentum=0.9, decay=0, nesterov=True) # sgd in general yields better results, but needs a lot of tweeking and is slower\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=[mape, smape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 5*24 # D -> 5\n",
    "num_features = data.shape[1] - 1\n",
    "output_dim = 1\n",
    "test_size = 0.9\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "X_train, y_train, X_test, y_test = df_to_lstm_format(df=data, \n",
    "                                                     test_size=test_size, \n",
    "                                                     look_back=look_back, \n",
    "                                                     target_column='gasPower', scale_X=True)\n",
    "\n",
    "# Create encoder, decoder i/o\n",
    "encoder_input_train = X_train\n",
    "decoder_input_train = np.zeros((X_train.shape[0], X_train.shape[1], 1)) # Zeros array\n",
    "decoder_output_train = y_train.reshape((y_train.shape[0], 1, 1))\n",
    "\n",
    "encoder_input_test = X_test\n",
    "decoder_input_test = np.zeros((X_test.shape[0], X_test.shape[1], 1)) # Zeros array\n",
    "decoder_output_test = y_test.reshape((y_test.shape[0], 1, 1))\n",
    "\n",
    "# Fit model\n",
    "early_stopping_monitor = EarlyStopping(patience=1000)\n",
    "\n",
    "#TODO: Check if data is being prosessed correclty. Results are the same-ish as with regular LSTM\n",
    "\n",
    "model.fit([encoder_input_train, decoder_input_train], decoder_output_train, \n",
    "          validation_data=([encoder_input_test, decoder_input_test], decoder_output_test), \n",
    "          epochs=epochs, verbose=2, callbacks=[PlotLossesKeras(), early_stopping_monitor])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_predict_model = keras.models.Model(encoder_inputs,\n",
    "                                           encoder_states)\n",
    "\n",
    "decoder_states_inputs = []\n",
    "\n",
    "# Read layers backwards to fit the format of initial_state\n",
    "# For some reason, the states of the model are order backwards (state of the first layer at the end of the list)\n",
    "# If instead of a GRU you were using an LSTM Cell, you would have to append two Input tensors since the LSTM has 2 states.\n",
    "for hidden_neurons in layers[::-1]:\n",
    "    # One state for GRU\n",
    "    decoder_states_inputs.append(keras.layers.Input(shape=(hidden_neurons,)))\n",
    "\n",
    "decoder_outputs_and_states = decoder(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_outputs = decoder_outputs_and_states[0]\n",
    "decoder_states = decoder_outputs_and_states[1:]\n",
    "\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_predict_model = keras.models.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, encoder_predict_model, decoder_predict_model, num_steps_to_predict):\n",
    "    \"\"\"Predict time series with encoder-decoder.\n",
    "    \n",
    "    Uses the encoder and decoder models previously trained to predict the next\n",
    "    num_steps_to_predict values of the time series.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    x: input time series of shape (batch_size, input_sequence_length, input_dimension).\n",
    "    encoder_predict_model: The Keras encoder model.\n",
    "    decoder_predict_model: The Keras decoder model.\n",
    "    num_steps_to_predict: The number of steps in the future to predict\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    y_predicted: output time series for shape (batch_size, target_sequence_length,\n",
    "        ouput_dimension)\n",
    "    \"\"\"\n",
    "    y_predicted = []\n",
    "\n",
    "    # Encode the values as a state vector\n",
    "    states = encoder_predict_model.predict(x)\n",
    "\n",
    "    # The states must be a list\n",
    "    if not isinstance(states, list):\n",
    "        states = [states]\n",
    "\n",
    "    # Generate first value of the decoder input sequence\n",
    "    decoder_input = np.zeros((x.shape[0], 1, 1))\n",
    "\n",
    "\n",
    "    for _ in range(num_steps_to_predict):\n",
    "        outputs_and_states = decoder_predict_model.predict(\n",
    "        [decoder_input] + states, batch_size=batch_size)\n",
    "        output = outputs_and_states[0]\n",
    "        states = outputs_and_states[1:]\n",
    "\n",
    "        # add predicted value\n",
    "        y_predicted.append(output)\n",
    "\n",
    "    return np.concatenate(y_predicted, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps_to_predict = 1\n",
    "batch_size = 1\n",
    "\n",
    "y_preds= predict(X_test, encoder_predict_model, decoder_predict_model, num_steps_to_predict)\n",
    "y_preds = y_preds.reshape(y_preds.shape[0])\n",
    "y_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test.reshape(y_test.shape[0])\n",
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(data.shape[0]*test_size)\n",
    "x = data[split_index:]\n",
    "#len(y_true), len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_difference = len(x) - len(y_true)\n",
    "x = x[datetime_difference:] # Correct for datetime difference, this is a dirty way of doing it\n",
    "x.index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(x.index, y_true, '.-', color='red', label='Real values', alpha=0.5)\n",
    "plt.plot(x.index, y_preds, '.-', color='blue', label='Predicted values')\n",
    "\n",
    "plt.ylabel(r'gasPower $\\cdot$ 10$^{-%s}$ [m$^3$/h]' % magnitude, fontsize=14)\n",
    "plt.xlabel('datetime [-]', fontsize=14) #TODO: set x values as actual dates\n",
    "\n",
    "plt.legend(loc='upper left', borderaxespad=0, frameon=False, fontsize=14, markerscale=3)\n",
    "\n",
    "mse_result = model.evaluate([encoder_input_test, decoder_input_test], decoder_output_test)[0]\n",
    "mape_result = model.evaluate([encoder_input_test, decoder_input_test], decoder_output_test)[1]\n",
    "smape_result = model.evaluate([encoder_input_test, decoder_input_test], decoder_output_test)[2]\n",
    "\n",
    "plt.title('Sequence to sequence GNU result \\n MSE = %.2f \\n MAPE = %.1f [%%] \\n SMAPE = %.1f [%%]' % (mse_result, mape_result, smape_result), fontsize = 14)\n",
    "\n",
    "#plt.savefig('figures/seq2seq result hourly 29-10-2018.png', dpi=1200)\n",
    "print('FINISHED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downsample to a day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(data.shape[0]*test_size)\n",
    "x = data[split_index:]\n",
    "#len(y_true), len(x)\n",
    "\n",
    "datetime_difference = len(x) - len(y_true)\n",
    "x = x[datetime_difference:] # Correct for datetime difference, this is a dirty way of doing it\n",
    "\n",
    "# Make it a df to be able to downsample\n",
    "datetime = x.index\n",
    "print(datetime.shape)\n",
    "\n",
    "y_preds = y_preds.reshape(y_preds.shape[0])\n",
    "y_true = y_true.reshape(y_true.shape[0])\n",
    "\n",
    "results = pd.DataFrame(y_true, y_preds) # For some reason y_true becomes the index\n",
    "result = results.reset_index() # Ugly way to fix above problem\n",
    "result.columns = ['y_pred', 'y_true']\n",
    "\n",
    "result['datetime'] = datetime\n",
    "result = result.set_index(['datetime'])\n",
    "\n",
    "result = result.resample('D').sum() # Resample data\n",
    "\n",
    "result = result.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics over the result\n",
    "\n",
    "ytrue = result['y_true']\n",
    "ypred = result['y_pred']\n",
    "n = len(result)\n",
    "\n",
    "mse_result = (1/n)*np.sum((ypred - ytrue)**2)\n",
    "mape_result = (100/n) * np.sum(np.abs((ytrue - ypred) / ypred))\n",
    "smape_result = (100/n) * np.sum( np.abs((ytrue - ypred)) / (np.abs(ytrue) + np.abs(ypred)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(result.index, result['y_true'], '.-', color='red', label='Real values', alpha=0.5, ms=10) # ms is markersize\n",
    "plt.plot(result.index, result['y_pred'], '.-', color='blue', label='Predicted values', ms=10)\n",
    "\n",
    "plt.ylabel(r'gasPower $\\cdot$ 10$^{-%s}$ [m$^3$/h]' % magnitude, fontsize=14)\n",
    "plt.xlabel('datetime [-]', fontsize=14) #TODO: set x values as actual dates\n",
    "\n",
    "plt.legend(loc='upper left', borderaxespad=0, frameon=False, fontsize=14, markerscale=3)\n",
    "\n",
    "plt.title('Sequence to sequence GNU result \\n MSE = %.2f \\n MAPE = %.1f [%%] \\n SMAPE = %.1f [%%]' % (mse_result, mape_result, smape_result), fontsize = 14)\n",
    "\n",
    "#plt.savefig('figures/LSTM result hourly resampled to daily by sum.png', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
