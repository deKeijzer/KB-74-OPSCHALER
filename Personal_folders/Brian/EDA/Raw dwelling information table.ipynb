{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unprocessed dwelling information extractor\n",
    "This notebook extracts usefull NaN information per dwelling and saves this into one file (csv & excel.  \n",
    "It will this for both the 10s and hour sample rate dataframes.  \n",
    "The unprocessed dataframes are loaded from: `//datc//opschaler//combined_gas_smart_weather_dfs//unprocessed//`  \n",
    "The nan information per dwelling is loaded from: `//datc//opschaler//nan_information//`  \n",
    "The final product is saved in: `//datc//opschaler//dwelling_information//total_information//`  \n",
    "It contains the following information per dwelling, for both the 10s and one hour sample rate:  \n",
    "* dwelling id  \n",
    "Dwelling id of the house.\n",
    "  \n",
    "* recorded days  \n",
    "The length of the unprocessed dataframe in days.\n",
    "\n",
    "* start date  \n",
    "The start date of the unprocessed dataframe.\n",
    "\n",
    "\n",
    "* stop date  \n",
    "The stop date of the unprocessed dataframe.\n",
    "\n",
    "* total samples (per columnm in thousands)  \n",
    "Total amount of samples in thousands, per column.\n",
    "  \n",
    "* total NaN streaks  \n",
    "Total amount of NaN streaks.\n",
    "  \n",
    "* total NaN streaks > 2  \n",
    "Total amount of NaN streaks which are larger than 2.\n",
    "  \n",
    "* total NaNs [-]  \n",
    "Total amount of NaNs in the unprocessed dataframe.\n",
    "  \n",
    "* total NaNs [%]  \n",
    "Totals NaNs devided by the total samples.\n",
    "  \n",
    "* mean of NaNs  \n",
    "Mean of the amount of NaNs per NaN streak.\n",
    "  \n",
    "* median of NaNs  \n",
    "Median of the amount of NaNs per NaN streak.\n",
    "  \n",
    "* std of NaNs  \n",
    "Standard deviation of the amount of NaNs per NaN streak.\n",
    "  \n",
    "* first highest NaN streak (%)  \n",
    "Amount of NaNs from the first highest NaN streak, devided by the total samples.\n",
    "  \n",
    "* first highest NaN streak column  \n",
    "The name of the column where the first highest NaN streak is in.\n",
    "  \n",
    "* second highest NaN streak (%)  \n",
    "Amount of NaNs from the first highest NaN streak, devided by the total samples.\n",
    "  \n",
    "* second highest NaN streak column  \n",
    "The name of the column where the first highest NaN streak is in.\n",
    "  \n",
    "* third highest NaN streak (%)  \n",
    "Amount of NaNs from the first highest NaN streak, devided by the total samples.\n",
    "  \n",
    "* third highest NaN streak column  \n",
    "The name of the column where the first highest NaN streak is in.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unprocessed_length_in_days(dwelling_id, type_):\n",
    "    \"\"\"\n",
    "    Get the total amount of days of the unprocessed dwelling_id.\n",
    "    \"\"\"\n",
    "    dir = '//datc//opschaler//combined_gas_smart_weather_dfs//unprocessed//'\n",
    "    df = pd.read_csv(dir+dwelling_id+'_'+type_+'.csv', delimiter='\\t', parse_dates=['datetime'])\n",
    "    columns = df.columns\n",
    "    df = df['datetime'] # only keep the datetime column\n",
    "    start_date = df.iloc[0]\n",
    "    stop_date = df.iloc[-1]\n",
    "    \n",
    "    del df # Free up memory\n",
    "    \n",
    "    recorded_days = (stop_date - start_date).days # total amount of recorded days\n",
    "\n",
    "    return recorded_days, start_date, stop_date, columns\n",
    "\n",
    "\n",
    "def nan_information_extractor(dwelling_id, path, type_):\n",
    "    \"\"\"\n",
    "    Extracts usefull information from the nan info table from a dwelling id. \n",
    "    Output is a list with this information.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, delimiter='\\t')\n",
    "    df = df.sort_values(by=['Amount of NaNs'], ascending=False) # Sort from highest to lowest amount of NaNs\n",
    "    \n",
    "    recorded_days, start_date, stop_date, columns = unprocessed_length_in_days(dwelling_id, type_) # Length of unprocessed dataframe in days\n",
    "    \n",
    "    if df.empty: # If df is empty, return nothins\n",
    "        #print('Dataframe is empty: %s' % path)\n",
    "        result = list(np.full(15, np.NaN)) # Make all outputs NaN\n",
    "        result[0] = dwelling_id\n",
    "        result[1] = recorded_days\n",
    "        return result\n",
    "    else:\n",
    "\n",
    "        if type_ == 'hour':\n",
    "            length = recorded_days*24\n",
    "        elif type_ == '10s':\n",
    "            length = recorded_days*24*60*6\n",
    "        else: \n",
    "            print('type_ must be \\'hour\\' or \\'10s')\n",
    "            \n",
    "        if length == 0:\n",
    "            result = list(np.full(15, np.NaN)) # Make all outputs NaN\n",
    "            result[0] = dwelling_id\n",
    "            result[1] = recorded_days\n",
    "            return result\n",
    "    \n",
    "        # Calculate usefull information\n",
    "        #total_samples = length*len(columns) # get the total amount of samples in the complete df\n",
    "        total_samples = length\n",
    "        total_gaps = len(df['Amount of NaNs'])\n",
    "        total_gaps_larger_than_2 = len(df[df['Amount of NaNs'] > 2])\n",
    "        total_nans = df['Amount of NaNs'].sum()\n",
    "        total_nans_percentage = (total_nans / total_samples)*100\n",
    "        mean = df['Amount of NaNs'].mean()\n",
    "        median = df['Amount of NaNs'].median()\n",
    "        std = df['Amount of NaNs'].std()\n",
    "        \n",
    "        # Try to get relevant values for the top 3 of NaN streaks\n",
    "        # Problem with this is that often there are multiple columns which have the same NaN streak...\n",
    "        try: \n",
    "            first_highest_p = (df['Amount of NaNs'][0]/ total_samples)*100\n",
    "            first_highest_column = df['Column name'][0]\n",
    "        except:\n",
    "            #print('There is no 1st highest')\n",
    "            first_highest_p = np.NaN\n",
    "            first_highest_column = np.NaN\n",
    "        \n",
    "        try:\n",
    "            second_highest_p = (df['Amount of NaNs'][1]/ total_samples)*100\n",
    "            second_highest_column = df['Column name'][1]\n",
    "        except:\n",
    "            #print('There is no 2nd highest')\n",
    "            second_highest_p = np.NaN\n",
    "            second_highest_column = np.NaN\n",
    "        \n",
    "        try:\n",
    "            third_highest_p = (df['Amount of NaNs'][3]/ total_samples)*100\n",
    "            third_highest_column = df['Column name'][3]\n",
    "        except:\n",
    "            #print('There is no 3rd highest')\n",
    "            third_highest_p = np.NaN\n",
    "            third_highest_column = np.NaN\n",
    "        \n",
    "    \n",
    "        # Put the results in a list\n",
    "        result = [dwelling_id, recorded_days, start_date, stop_date, (total_samples/1000), total_gaps, total_gaps_larger_than_2, total_nans, total_nans_percentage, mean, median, \n",
    "                  std, first_highest_p, first_highest_column, second_highest_p, second_highest_column, third_highest_p, third_highest_column]\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    nan_dir = '//datc//opschaler//nan_information//'\n",
    "    paths_h = glob.glob(nan_dir+'*_hour.csv')\n",
    "    ids_h = list(map(lambda x: x[-20:-9], paths_h))\n",
    "    \n",
    "    paths_s = glob.glob(nan_dir+'*_10s.csv')\n",
    "    ids_s = list(map(lambda x: x[-20:-8], paths_s))\n",
    "    \n",
    "    results_h = []\n",
    "    results_10s = []\n",
    "    \n",
    "    headers=['dwelling id', 'recorded days', 'start date', 'stop date', 'total samples (per column, in thousands)','total NaN streaks', 'total NaN streaks > 2','total NaNs [-]', 'total NaNs [%]', 'mean of NaNs', 'median of NaNs', 'std of NaNs', \n",
    "             'first highest NaN streak (%)', 'first highest NaN streak column', \n",
    "             'second highest NaN streak (%)', 'second highest NaN streak column', \n",
    "             'third highest NaN streak (%)', 'third highest NaN streak column']\n",
    "    \n",
    "    for i, path in enumerate(tqdm(paths_h)):\n",
    "        dwelling_id = ids_h[i]\n",
    "        type_ = 'hour'\n",
    "        results_h.append(nan_information_extractor(dwelling_id, path, type_))\n",
    "    \n",
    "    for i, path in enumerate(tqdm(paths_s)):\n",
    "        dwelling_id = ids_s[i]\n",
    "        type_ = '10s'\n",
    "        results_10s.append(nan_information_extractor(dwelling_id, path, type_))\n",
    "        \n",
    "    # make df from list of lists, round all values within to 1 decimal.\n",
    "    df_hour = pd.DataFrame.from_records(results_h, columns=headers).round(decimals=5) \n",
    "    df_10s = pd.DataFrame.from_records(results_10s, columns=headers).round(decimals=5) \n",
    "    \n",
    "    # round some numbers differently\n",
    "    one_decimal = ['total samples (per column, in thousands)', 'total NaNs [%]', 'mean of NaNs', 'std of NaNs']\n",
    "    df_hour[one_decimal] = df_hour[one_decimal].round(decimals=1) \n",
    "    df_10s[one_decimal] = df_10s[one_decimal].round(decimals=1) \n",
    "    \n",
    "    \n",
    "    \n",
    "    # sort by recoded days, highest to lowest\n",
    "    df_hour = df_hour.sort_values(by=['recorded days'], ascending=False)\n",
    "    df_10s = df_10s.sort_values(by=['recorded days'], ascending=False)\n",
    "    \n",
    "    return df_10s, df_hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run main() and save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [00:00<00:00, 83.08it/s]\n",
      "100%|██████████| 56/56 [02:24<00:00,  3.00s/it]\n"
     ]
    }
   ],
   "source": [
    "info_10s, info_hour = main() # This takes ~2,5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED\n"
     ]
    }
   ],
   "source": [
    "info_hour.to_csv('//datc//opschaler//dwelling_information//total_information//total_nan_information_hour.csv', sep='\\t', index=False)\n",
    "info_10s.to_csv('//datc//opschaler//dwelling_information//total_information//nan_information_10s.csv', sep='\\t', index=False)\n",
    "\n",
    "# Also save to Excel\n",
    "writer = pd.ExcelWriter('//datc//opschaler//dwelling_information//total_information//total_nan_information.xlsx')\n",
    "info_hour.to_excel(writer,'Hour dataframes', index=False)\n",
    "info_10s.to_excel(writer,'10s dataframes', index=False)\n",
    "writer.save()\n",
    "\n",
    "print('FINISHED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
