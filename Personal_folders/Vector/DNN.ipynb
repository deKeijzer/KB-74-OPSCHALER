{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "#copypasted -> for gpu usage\n",
    "import tensorflow as tf\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "from tensorflow.python.client import device_lib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of GPUs available: 4\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: Tells tf to not occupy a specific amount of memory\n",
    "from keras.backend.tensorflow_backend import set_session  \n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU  \n",
    "sess = tf.Session(config=config)  \n",
    "set_session(sess)  # set this TensorFlow session as the default session for Keras.\n",
    "\n",
    "\n",
    "# getting the number of GPUs \n",
    "def get_available_gpus():\n",
    "   local_device_protos = device_lib.list_local_devices()\n",
    "   return [x.name for x in local_device_protos if x.device_type    == 'GPU']\n",
    "num_gpu = len(get_available_gpus())\n",
    "print('Amount of GPUs available: %s' % num_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data and make it usable\n",
    "df = pd.read_csv('//datc//opschaler//combined_gas_smart_weather_dfs//processed//all_dwellings_combined_hour.csv', delimiter='\\t', parse_dates=['datetime'])\n",
    "df = df.set_index(['datetime'])\n",
    "df = df.dropna()\n",
    "df['gasPower'] = df['gasPower']*10\n",
    "df['T-1'] = df['T'].shift(1)\n",
    "df_d = df.resample('1D').mean() #Resampling to a day\n",
    "df_d = df_d.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add dummies!\n",
    "df_d['hour'] = df_d.index.hour #create dummy variable (hour)\n",
    "df_d['dayofweek'] = df_d.index.dayofweek #Create dummy variable (day of the week)\n",
    "columns_to_cat = ['hour', 'dayofweek']\n",
    "df_d[columns_to_cat] = df_d[columns_to_cat].astype('category') # change datetypes->category\n",
    "df_d = pd.get_dummies(df_d, columns=columns_to_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/jupyterhub/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype uint8, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "#Split, originally thought about using the train_test_split function but it seems to give some errors\n",
    "X = df_d[['T','T-1','dayofweek_0','dayofweek_1','dayofweek_2','dayofweek_3','dayofweek_4','dayofweek_5','dayofweek_6']]\n",
    "y = df_d[['gasPower']]\n",
    "labelencoder_y_1 = LabelEncoder()\n",
    "y = labelencoder_y_1.fit_transform(y)\n",
    "\n",
    "test_size=0.3\n",
    "split_index = int(df_d.shape[0]*test_size)\n",
    "\n",
    "X_train = X[:split_index]\n",
    "y_train = y[:split_index]\n",
    "X_test = X[split_index:]\n",
    "y_test = y[split_index:]\n",
    "\n",
    "scalerX = StandardScaler(with_mean=True, with_std=True).fit(X_train) #Normalization! :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/jupyterhub/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype uint8, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/opt/jupyterhub/anaconda/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype uint8, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/opt/jupyterhub/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:3: DataConversionWarning: Data with input dtype uint8, float64 were all converted to float64 by StandardScaler.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/jupyterhub/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=9, units=16, kernel_initializer=\"uniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/opt/jupyterhub/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=16, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/opt/jupyterhub/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "forecast = Sequential()\n",
    "\n",
    "#first layer with the input data\n",
    "forecast.add(Dense(output_dim =16, init = 'uniform', activation = 'relu', input_dim = 9))\n",
    "#second layer \n",
    "forecast.add(Dense(output_dim = 16, init = 'uniform', activation = 'relu'))\n",
    "#output layer\n",
    "forecast.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_model = multi_gpu_model(forecast, gpus=num_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_model.compile(optimizer = 'adam', loss = 'mse', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "83/83 [==============================] - 2s 26ms/step - loss: 0.0669 - acc: 0.0120\n",
      "Epoch 2/100\n",
      "83/83 [==============================] - 0s 394us/step - loss: 0.0658 - acc: 0.0120\n",
      "Epoch 3/100\n",
      "83/83 [==============================] - 0s 361us/step - loss: 0.0645 - acc: 0.0120\n",
      "Epoch 4/100\n",
      "83/83 [==============================] - 0s 427us/step - loss: 0.0628 - acc: 0.0120\n",
      "Epoch 5/100\n",
      "83/83 [==============================] - 0s 370us/step - loss: 0.0604 - acc: 0.0120\n",
      "Epoch 6/100\n",
      "83/83 [==============================] - 0s 372us/step - loss: 0.0574 - acc: 0.0120\n",
      "Epoch 7/100\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.0532 - acc: 0.0120\n",
      "Epoch 8/100\n",
      "83/83 [==============================] - 0s 703us/step - loss: 0.0484 - acc: 0.0120\n",
      "Epoch 9/100\n",
      "83/83 [==============================] - 0s 556us/step - loss: 0.0432 - acc: 0.0120\n",
      "Epoch 10/100\n",
      "83/83 [==============================] - 0s 369us/step - loss: 0.0370 - acc: 0.0120\n",
      "Epoch 11/100\n",
      "83/83 [==============================] - 0s 373us/step - loss: 0.0325 - acc: 0.0120\n",
      "Epoch 12/100\n",
      "83/83 [==============================] - 0s 347us/step - loss: 0.0291 - acc: 0.0120\n",
      "Epoch 13/100\n",
      "83/83 [==============================] - 0s 343us/step - loss: 0.0266 - acc: 0.0120\n",
      "Epoch 14/100\n",
      "83/83 [==============================] - 0s 358us/step - loss: 0.0254 - acc: 0.0120\n",
      "Epoch 15/100\n",
      "83/83 [==============================] - 0s 348us/step - loss: 0.0247 - acc: 0.0120\n",
      "Epoch 16/100\n",
      "83/83 [==============================] - 0s 366us/step - loss: 0.0243 - acc: 0.0120\n",
      "Epoch 17/100\n",
      "83/83 [==============================] - 0s 358us/step - loss: 0.0240 - acc: 0.0120\n",
      "Epoch 18/100\n",
      "83/83 [==============================] - 0s 373us/step - loss: 0.0237 - acc: 0.0120\n",
      "Epoch 19/100\n",
      "83/83 [==============================] - 0s 372us/step - loss: 0.0234 - acc: 0.0120\n",
      "Epoch 20/100\n",
      "83/83 [==============================] - 0s 353us/step - loss: 0.0233 - acc: 0.0120\n",
      "Epoch 21/100\n",
      "83/83 [==============================] - 0s 347us/step - loss: 0.0231 - acc: 0.0120\n",
      "Epoch 22/100\n",
      "83/83 [==============================] - 0s 349us/step - loss: 0.0229 - acc: 0.0120\n",
      "Epoch 23/100\n",
      "83/83 [==============================] - 0s 350us/step - loss: 0.0228 - acc: 0.0120\n",
      "Epoch 24/100\n",
      "83/83 [==============================] - 0s 348us/step - loss: 0.0227 - acc: 0.0120\n",
      "Epoch 25/100\n",
      "83/83 [==============================] - 0s 350us/step - loss: 0.0226 - acc: 0.0120\n",
      "Epoch 26/100\n",
      "83/83 [==============================] - 0s 375us/step - loss: 0.0226 - acc: 0.0120\n",
      "Epoch 27/100\n",
      "83/83 [==============================] - 0s 381us/step - loss: 0.0225 - acc: 0.0120\n",
      "Epoch 28/100\n",
      "83/83 [==============================] - 0s 374us/step - loss: 0.0224 - acc: 0.0120\n",
      "Epoch 29/100\n",
      "83/83 [==============================] - 0s 355us/step - loss: 0.0223 - acc: 0.0120\n",
      "Epoch 30/100\n",
      "83/83 [==============================] - 0s 342us/step - loss: 0.0223 - acc: 0.0120\n",
      "Epoch 31/100\n",
      "83/83 [==============================] - 0s 348us/step - loss: 0.0223 - acc: 0.0120\n",
      "Epoch 32/100\n",
      "83/83 [==============================] - 0s 363us/step - loss: 0.0222 - acc: 0.0120\n",
      "Epoch 33/100\n",
      "83/83 [==============================] - 0s 346us/step - loss: 0.0221 - acc: 0.0120\n",
      "Epoch 34/100\n",
      "83/83 [==============================] - 0s 345us/step - loss: 0.0221 - acc: 0.0120\n",
      "Epoch 35/100\n",
      "83/83 [==============================] - 0s 357us/step - loss: 0.0220 - acc: 0.0120\n",
      "Epoch 36/100\n",
      "83/83 [==============================] - 0s 342us/step - loss: 0.0220 - acc: 0.0120\n",
      "Epoch 37/100\n",
      "83/83 [==============================] - 0s 363us/step - loss: 0.0219 - acc: 0.0120\n",
      "Epoch 38/100\n",
      "83/83 [==============================] - 0s 348us/step - loss: 0.0219 - acc: 0.0120\n",
      "Epoch 39/100\n",
      "83/83 [==============================] - 0s 369us/step - loss: 0.0219 - acc: 0.0120\n",
      "Epoch 40/100\n",
      "83/83 [==============================] - 0s 360us/step - loss: 0.0218 - acc: 0.0120\n",
      "Epoch 41/100\n",
      "83/83 [==============================] - 0s 352us/step - loss: 0.0218 - acc: 0.0120\n",
      "Epoch 42/100\n",
      "83/83 [==============================] - 0s 382us/step - loss: 0.0218 - acc: 0.0120\n",
      "Epoch 43/100\n",
      "83/83 [==============================] - 0s 357us/step - loss: 0.0217 - acc: 0.0120\n",
      "Epoch 44/100\n",
      "83/83 [==============================] - 0s 347us/step - loss: 0.0217 - acc: 0.0120\n",
      "Epoch 45/100\n",
      "83/83 [==============================] - 0s 358us/step - loss: 0.0217 - acc: 0.0120\n",
      "Epoch 46/100\n",
      "83/83 [==============================] - 0s 359us/step - loss: 0.0216 - acc: 0.0120\n",
      "Epoch 47/100\n",
      "83/83 [==============================] - 0s 374us/step - loss: 0.0216 - acc: 0.0120\n",
      "Epoch 48/100\n",
      "83/83 [==============================] - 0s 360us/step - loss: 0.0216 - acc: 0.0120\n",
      "Epoch 49/100\n",
      "83/83 [==============================] - 0s 353us/step - loss: 0.0215 - acc: 0.0120\n",
      "Epoch 50/100\n",
      "83/83 [==============================] - 0s 370us/step - loss: 0.0215 - acc: 0.0120\n",
      "Epoch 51/100\n",
      "83/83 [==============================] - 0s 341us/step - loss: 0.0215 - acc: 0.0120\n",
      "Epoch 52/100\n",
      "83/83 [==============================] - 0s 356us/step - loss: 0.0215 - acc: 0.0120\n",
      "Epoch 53/100\n",
      "83/83 [==============================] - 0s 357us/step - loss: 0.0215 - acc: 0.0120\n",
      "Epoch 54/100\n",
      "83/83 [==============================] - 0s 343us/step - loss: 0.0214 - acc: 0.0120\n",
      "Epoch 55/100\n",
      "83/83 [==============================] - 0s 343us/step - loss: 0.0213 - acc: 0.0120\n",
      "Epoch 56/100\n",
      "83/83 [==============================] - 0s 348us/step - loss: 0.0214 - acc: 0.0120\n",
      "Epoch 57/100\n",
      "83/83 [==============================] - 0s 344us/step - loss: 0.0213 - acc: 0.0120\n",
      "Epoch 58/100\n",
      "83/83 [==============================] - 0s 341us/step - loss: 0.0213 - acc: 0.0120\n",
      "Epoch 59/100\n",
      "83/83 [==============================] - 0s 372us/step - loss: 0.0212 - acc: 0.0120\n",
      "Epoch 60/100\n",
      "83/83 [==============================] - 0s 347us/step - loss: 0.0213 - acc: 0.0120\n",
      "Epoch 61/100\n",
      "83/83 [==============================] - 0s 358us/step - loss: 0.0212 - acc: 0.0120\n",
      "Epoch 62/100\n",
      "83/83 [==============================] - 0s 374us/step - loss: 0.0211 - acc: 0.0120\n",
      "Epoch 63/100\n",
      "83/83 [==============================] - 0s 376us/step - loss: 0.0211 - acc: 0.0120\n",
      "Epoch 64/100\n",
      "83/83 [==============================] - 0s 343us/step - loss: 0.0211 - acc: 0.0120\n",
      "Epoch 65/100\n",
      "83/83 [==============================] - 0s 373us/step - loss: 0.0210 - acc: 0.0120\n",
      "Epoch 66/100\n",
      "83/83 [==============================] - 0s 367us/step - loss: 0.0210 - acc: 0.0120\n",
      "Epoch 67/100\n",
      "83/83 [==============================] - 0s 359us/step - loss: 0.0210 - acc: 0.0120\n",
      "Epoch 68/100\n",
      "83/83 [==============================] - 0s 342us/step - loss: 0.0209 - acc: 0.0120\n",
      "Epoch 69/100\n",
      "83/83 [==============================] - 0s 357us/step - loss: 0.0209 - acc: 0.0120\n",
      "Epoch 70/100\n",
      "83/83 [==============================] - 0s 348us/step - loss: 0.0209 - acc: 0.0120\n",
      "Epoch 71/100\n",
      "83/83 [==============================] - 0s 369us/step - loss: 0.0208 - acc: 0.0120\n",
      "Epoch 72/100\n",
      "83/83 [==============================] - 0s 343us/step - loss: 0.0208 - acc: 0.0120\n",
      "Epoch 73/100\n",
      "83/83 [==============================] - 0s 356us/step - loss: 0.0208 - acc: 0.0120\n",
      "Epoch 74/100\n",
      "83/83 [==============================] - 0s 345us/step - loss: 0.0207 - acc: 0.0120\n",
      "Epoch 75/100\n",
      "83/83 [==============================] - 0s 372us/step - loss: 0.0206 - acc: 0.0120\n",
      "Epoch 76/100\n",
      "83/83 [==============================] - 0s 359us/step - loss: 0.0206 - acc: 0.0120\n",
      "Epoch 77/100\n",
      "83/83 [==============================] - 0s 347us/step - loss: 0.0203 - acc: 0.0120\n",
      "Epoch 78/100\n",
      "83/83 [==============================] - 0s 358us/step - loss: 0.0202 - acc: 0.0120\n",
      "Epoch 79/100\n",
      "83/83 [==============================] - 0s 344us/step - loss: 0.0200 - acc: 0.0120\n",
      "Epoch 80/100\n",
      "83/83 [==============================] - 0s 372us/step - loss: 0.0199 - acc: 0.0120\n",
      "Epoch 81/100\n",
      "83/83 [==============================] - 0s 344us/step - loss: 0.0198 - acc: 0.0120\n",
      "Epoch 82/100\n",
      "83/83 [==============================] - 0s 346us/step - loss: 0.0196 - acc: 0.0120\n",
      "Epoch 83/100\n",
      "83/83 [==============================] - 0s 366us/step - loss: 0.0194 - acc: 0.0120\n",
      "Epoch 84/100\n",
      "83/83 [==============================] - 0s 371us/step - loss: 0.0192 - acc: 0.0120\n",
      "Epoch 85/100\n",
      "83/83 [==============================] - 0s 347us/step - loss: 0.0191 - acc: 0.0120\n",
      "Epoch 86/100\n",
      "83/83 [==============================] - 0s 346us/step - loss: 0.0190 - acc: 0.0120\n",
      "Epoch 87/100\n",
      "83/83 [==============================] - 0s 346us/step - loss: 0.0188 - acc: 0.0120\n",
      "Epoch 88/100\n",
      "83/83 [==============================] - 0s 355us/step - loss: 0.0187 - acc: 0.0120\n",
      "Epoch 89/100\n",
      "83/83 [==============================] - 0s 346us/step - loss: 0.0186 - acc: 0.0120\n",
      "Epoch 90/100\n",
      "83/83 [==============================] - 0s 345us/step - loss: 0.0183 - acc: 0.0120\n",
      "Epoch 91/100\n",
      "83/83 [==============================] - 0s 367us/step - loss: 0.0182 - acc: 0.0120\n",
      "Epoch 92/100\n",
      "83/83 [==============================] - 0s 379us/step - loss: 0.0181 - acc: 0.0120\n",
      "Epoch 93/100\n",
      "83/83 [==============================] - 0s 347us/step - loss: 0.0181 - acc: 0.0120\n",
      "Epoch 94/100\n",
      "83/83 [==============================] - 0s 346us/step - loss: 0.0179 - acc: 0.0120\n",
      "Epoch 95/100\n",
      "83/83 [==============================] - 0s 356us/step - loss: 0.0178 - acc: 0.0120\n",
      "Epoch 96/100\n",
      "83/83 [==============================] - 0s 355us/step - loss: 0.0178 - acc: 0.0120\n",
      "Epoch 97/100\n",
      "83/83 [==============================] - 0s 370us/step - loss: 0.0177 - acc: 0.0120\n",
      "Epoch 98/100\n",
      "83/83 [==============================] - 0s 361us/step - loss: 0.0177 - acc: 0.0120\n",
      "Epoch 99/100\n",
      "83/83 [==============================] - 0s 347us/step - loss: 0.0177 - acc: 0.0120\n",
      "Epoch 100/100\n",
      "83/83 [==============================] - 0s 355us/step - loss: 0.0175 - acc: 0.0120\n"
     ]
    }
   ],
   "source": [
    "y_train = (y_train-min(y_train))/(max(y_train)-min(y_train))\n",
    "\n",
    "forecast_model.fit(X_train, y_train, batch_size = 10,epochs = 100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = forecast_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
