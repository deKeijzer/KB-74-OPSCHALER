{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "global gas\n",
    "global info_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_dir = '//datc//opschaler//combined_gas_smart_weather_dfs//unprocessed//'\n",
    "nan_info_dir = '//datc//opschaler//nan_information//'\n",
    "paths_h = glob.glob(unprocessed_dir+'P*_hour.csv') #Getting all files matching that path + expression \n",
    "dwelling_ids = np.array(list((map(lambda x: x[-20::], paths_h))))\n",
    "#paths_s = glob.glob(unprocessed_dir+'*_10s.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code is used to extract information about gaps size, location of first big nan gap \n",
    "#(useful for periodicity porpouse and identify problems in acquisition system)\n",
    "def extract_info(dwelling_ids):\n",
    "    \n",
    "    info_data = pd.DataFrame(columns={'House_ID','Initial_Date','Final_Date','Days','Amount_of_NaNs','Max_Consecutive_NaNs','First_Gap_Valid_Data'})\n",
    "    for dwelling in tqdm(dwelling_ids) : #Loop over all dwellings\n",
    "                \n",
    "        df=pd.read_csv(unprocessed_dir+dwelling, delimiter='\\t', parse_dates=['datetime'])\n",
    "        df = df.set_index(['datetime'])\n",
    "        df_nan=df.gasMeter.isnull()\n",
    "        temp = pd.DataFrame()\n",
    "        temp['temp']=df_nan.groupby((df_nan != df_nan.shift()).cumsum()).transform('size') * df_nan \n",
    "        max_gap=temp['temp'].max()\n",
    " \n",
    "        nan_table=pd.read_csv(nan_info_dir+dwelling, delimiter='\\t')\n",
    "        nan_table=nan_table.loc[nan_table['Column name'] == 'gasMeter']\n",
    "        nan_table.set_index('Unnamed: 0')\n",
    "        first_big_nan_index=nan_table[nan_table['Amount of NaNs']>=5].index\n",
    "        \n",
    "        if first_big_nan_index.empty:\n",
    "            valid_data='All_file_good'\n",
    "        else :\n",
    "            valid_data=datetime.strptime(nan_table['Start index'].loc[first_big_nan_index[0]],'%Y-%m-%d %H:%M:%S')-df.index[0]\n",
    "            \n",
    "        #Extract information about dates and number of NaNs\n",
    "        initial_date_string= df.index[0].strftime('%Y-%m-%d %H:%M:%S')\n",
    "        final_date_string= df.index[-1].strftime('%Y-%m-%d %H:%M:%S')\n",
    "        days=df.index[-1]-df.index[0]\n",
    "        NaN_count=df.gasMeter.isnull().sum()\n",
    "        \n",
    "        temp={'House_ID':dwelling,'Initial_Date':initial_date_string,'Final_Date':final_date_string,'Days':days,'Amount_of_NaNs':NaN_count,'Max_Consecutive_NaNs':max_gap,'First_Gap_Valid_Data':valid_data}\n",
    "        #Append all information to the global info_data dataframe\n",
    "        info_data = info_data.append(temp, ignore_index=True)\n",
    "        #info_data.loc[len(info_data.index)] = [dwelling_name[-20:], initial_date_string, final_date_string, days, NaN_count, max_gap]\n",
    "    return (info_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This part is a combination with Brian de Keijzer notebook on removing NaNs and personalcode\n",
    "def drop_week_big_nan(dwelling_ids, gap_size):\n",
    "    \"\"\"\n",
    "    Drop whole weeks containing NaN gaps bigger than the gap_size specified in the input\n",
    "    :param df: Pandas DataDrame to process NaNs off\n",
    "    :param df_nan_table: NaN info Pandas DataFrame of the input df\n",
    "    :param gap_size: number of consecutive NaNs \n",
    "    :return: Pandas DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    for dwelling in tqdm(dwelling_ids):\n",
    "        \n",
    "        nan_table=pd.read_csv(nan_info_dir+dwelling, delimiter='\\t')\n",
    "        nan_table=nan_table.loc[nan_table['Column name'] == 'gasMeter']\n",
    "        big_nans_index=nan_table[nan_table['Amount of NaNs']>=gap_size].index\n",
    "        \n",
    "        df = pd.read_csv(unprocessed_dir+dwelling, delimiter='\\t', parse_dates=['datetime'])\n",
    "        df = df.set_index(['datetime'])\n",
    "        #print(big_nans_index)\n",
    "        \n",
    "        \n",
    "        if big_nans_index.empty:\n",
    "            pass\n",
    "        else :\n",
    "            \n",
    "            for index in big_nans_index:\n",
    "                dt=datetime.strptime(nan_table['Start index'].loc[index],'%Y-%m-%d %H:%M:%S')\n",
    "                start = dt - timedelta(days=dt.weekday())\n",
    "                start.replace(hour=0,minute=0,second=0)\n",
    "                start=start.date()\n",
    "                end = start + timedelta(days=7)             \n",
    "                start=df.index.searchsorted(start)\n",
    "                end=df.index.searchsorted(end)\n",
    "                \n",
    "                df.drop(df.index[start:end],inplace=True)\n",
    "            \n",
    "        dir = '//datc//opschaler//combined_gas_smart_weather_dfs//processed//Weeks_gas_drop//'\n",
    "        df.to_csv(dir +'weeks_removed_'+ dwelling, sep='\\t', index=True)  \n",
    " \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:03<00:00, 16.62it/s]\n"
     ]
    }
   ],
   "source": [
    "drop_week_big_nan(dwelling_ids,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:01<00:00, 45.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   First_Gap_Valid_Data           Final_Date              House_ID  \\\n",
      "0      28 days 03:00:00  2017-05-30 13:00:00  P01S01W7548_hour.csv   \n",
      "1      15 days 03:00:00  2017-11-21 18:00:00  P01S02W0167_hour.csv   \n",
      "2      28 days 11:00:00  2017-05-30 10:00:00  P01S01W5040_hour.csv   \n",
      "3      28 days 11:00:00  2017-05-29 13:00:00  P01S01W8669_hour.csv   \n",
      "4       0 days 18:00:00  2017-03-09 02:00:00  P01S01W0000_hour.csv   \n",
      "5      19 days 10:00:00  2017-06-09 10:00:00  P01S01W9617_hour.csv   \n",
      "6      28 days 11:00:00  2017-05-31 11:00:00  P01S01W5588_hour.csv   \n",
      "7      19 days 10:00:00  2017-05-29 16:00:00  P01S01W9431_hour.csv   \n",
      "8      28 days 11:00:00  2017-05-29 12:00:00  P01S01W4002_hour.csv   \n",
      "9         All_file_good  2017-06-02 14:00:00  P01S01W7042_hour.csv   \n",
      "10      7 days 14:00:00  2017-06-01 13:00:00  P01S01W6289_hour.csv   \n",
      "11     28 days 11:00:00  2017-05-29 18:00:00  P01S01W5476_hour.csv   \n",
      "12     17 days 00:00:00  2017-11-21 20:00:00  P01S02W4827_hour.csv   \n",
      "13        All_file_good  2017-05-31 17:00:00  P01S01W1554_hour.csv   \n",
      "14     19 days 10:00:00  2017-05-31 10:00:00  P01S01W8828_hour.csv   \n",
      "15     27 days 21:00:00  2017-12-01 08:00:00  P01S01W2743_hour.csv   \n",
      "16     81 days 02:00:00  2017-11-21 20:00:00  P01S01W5339_hour.csv   \n",
      "17     28 days 11:00:00  2017-05-31 11:00:00  P01S01W1341_hour.csv   \n",
      "18     28 days 11:00:00  2017-05-29 10:00:00  P01S01W5564_hour.csv   \n",
      "19     28 days 11:00:00  2017-05-29 19:00:00  P01S01W2581_hour.csv   \n",
      "20     28 days 00:00:00  2017-07-14 13:00:00  P01S01W5746_hour.csv   \n",
      "21      0 days 00:00:00  2017-05-29 16:00:00  P01S01W5292_hour.csv   \n",
      "22     81 days 02:00:00  2017-11-20 18:00:00  P01S01W8655_hour.csv   \n",
      "23      6 days 07:00:00  2017-03-30 22:00:00  P01S01W0998_hour.csv   \n",
      "24        All_file_good  2017-05-30 12:00:00  P01S01W4979_hour.csv   \n",
      "25     28 days 03:00:00  2017-05-29 11:00:00  P01S01W6959_hour.csv   \n",
      "26      6 days 07:00:00  2017-05-31 16:00:00  P01S01W4091_hour.csv   \n",
      "27     36 days 12:00:00  2017-12-01 09:00:00  P01S02W6848_hour.csv   \n",
      "28      0 days 18:00:00  2017-03-29 18:00:00  P01S01W0373_hour.csv   \n",
      "29      9 days 03:00:00  2017-11-23 15:00:00  P01S02W4953_hour.csv   \n",
      "30     19 days 10:00:00  2017-05-31 10:00:00  P01S01W6595_hour.csv   \n",
      "31     28 days 11:00:00  2017-05-29 17:00:00  P01S01W4489_hour.csv   \n",
      "32     28 days 11:00:00  2017-05-29 10:00:00  P01S01W5855_hour.csv   \n",
      "33     28 days 11:00:00  2017-05-30 11:00:00  P01S01W3955_hour.csv   \n",
      "34      0 days 10:00:00  2017-05-30 10:00:00  P01S01W4313_hour.csv   \n",
      "35     28 days 11:00:00  2017-05-29 11:00:00  P01S01W6835_hour.csv   \n",
      "36     28 days 11:00:00  2017-05-29 18:00:00  P01S01W8239_hour.csv   \n",
      "37     28 days 11:00:00  2017-05-29 12:00:00  P01S01W1347_hour.csv   \n",
      "38      2 days 21:00:00  2017-11-23 19:00:00  P01S02W7251_hour.csv   \n",
      "39     28 days 11:00:00  2017-06-09 10:00:00  P01S01W8171_hour.csv   \n",
      "40        All_file_good  2017-05-30 17:00:00  P01S01W7980_hour.csv   \n",
      "41     28 days 03:00:00  2017-06-02 14:00:00  P01S01W7071_hour.csv   \n",
      "42     28 days 11:00:00  2017-05-31 12:00:00  P01S01W3497_hour.csv   \n",
      "43     81 days 02:00:00  2017-11-22 21:00:00  P01S01W6495_hour.csv   \n",
      "44     17 days 00:00:00  2017-11-22 11:00:00  P01S02W2995_hour.csv   \n",
      "45     28 days 11:00:00  2017-06-01 12:00:00  P01S01W0378_hour.csv   \n",
      "46     28 days 11:00:00  2017-05-30 15:00:00  P01S01W4589_hour.csv   \n",
      "47        All_file_good  2017-03-07 07:00:00  P01S01W0001_hour.csv   \n",
      "48     28 days 11:00:00  2017-05-29 14:00:00  P01S01W4569_hour.csv   \n",
      "49     28 days 11:00:00  2017-05-31 09:00:00  P01S01W6549_hour.csv   \n",
      "50     28 days 11:00:00  2017-05-31 18:00:00  P01S01W4579_hour.csv   \n",
      "51     21 days 04:00:00  2017-11-22 11:00:00  P01S02W5065_hour.csv   \n",
      "\n",
      "                Days Amount_of_NaNs Max_Consecutive_NaNs         Initial_Date  \n",
      "0   83 days 22:00:00             68                   62  2017-03-07 15:00:00  \n",
      "1  145 days 07:00:00            935                  845  2017-06-29 11:00:00  \n",
      "2   84 days 03:00:00             67                   62  2017-03-07 07:00:00  \n",
      "3   83 days 06:00:00             65                   62  2017-03-07 07:00:00  \n",
      "4    1 days 19:00:00              6                    6  2017-03-07 07:00:00  \n",
      "5   85 days 02:00:00             71                   62  2017-03-16 08:00:00  \n",
      "6   85 days 04:00:00             66                   62  2017-03-07 07:00:00  \n",
      "7   74 days 08:00:00             66                   62  2017-03-16 08:00:00  \n",
      "8   83 days 05:00:00             64                   62  2017-03-07 07:00:00  \n",
      "9   39 days 02:00:00              0                    0  2017-04-24 12:00:00  \n",
      "10  86 days 06:00:00             89                   62  2017-03-07 07:00:00  \n",
      "11  83 days 11:00:00             68                   62  2017-03-07 07:00:00  \n",
      "12 147 days 06:00:00             87                   86  2017-06-27 14:00:00  \n",
      "13  27 days 06:00:00              0                    0  2017-05-04 11:00:00  \n",
      "14  76 days 02:00:00             64                   62  2017-03-16 08:00:00  \n",
      "15 220 days 20:00:00           1289                  604  2017-04-24 12:00:00  \n",
      "16 211 days 08:00:00            112                   86  2017-04-24 12:00:00  \n",
      "17  85 days 04:00:00             64                   62  2017-03-07 07:00:00  \n",
      "18  83 days 03:00:00             67                   62  2017-03-07 07:00:00  \n",
      "19  83 days 12:00:00             65                   62  2017-03-07 07:00:00  \n",
      "20 128 days 19:00:00             64                   62  2017-03-07 18:00:00  \n",
      "21  96 days 05:00:00            234                  144  2017-02-22 11:00:00  \n",
      "22 210 days 06:00:00            100                   86  2017-04-24 12:00:00  \n",
      "23  23 days 15:00:00             28                    7  2017-03-07 07:00:00  \n",
      "24  26 days 01:00:00              1                    1  2017-05-04 11:00:00  \n",
      "25  82 days 20:00:00             65                   62  2017-03-07 15:00:00  \n",
      "26  98 days 05:00:00            135                   71  2017-02-22 11:00:00  \n",
      "27  92 days 20:00:00            284                  264  2017-08-30 13:00:00  \n",
      "28  22 days 11:00:00             14                    6  2017-03-07 07:00:00  \n",
      "29 141 days 04:00:00            118                   86  2017-07-05 11:00:00  \n",
      "30  76 days 02:00:00             67                   62  2017-03-16 08:00:00  \n",
      "31  83 days 10:00:00             79                   62  2017-03-07 07:00:00  \n",
      "32  83 days 03:00:00             64                   62  2017-03-07 07:00:00  \n",
      "33  84 days 04:00:00             64                   62  2017-03-07 07:00:00  \n",
      "34  84 days 02:00:00            640                   62  2017-03-07 08:00:00  \n",
      "35  83 days 04:00:00             69                   62  2017-03-07 07:00:00  \n",
      "36  83 days 11:00:00             65                   62  2017-03-07 07:00:00  \n",
      "37  83 days 05:00:00             65                   62  2017-03-07 07:00:00  \n",
      "38 146 days 08:00:00           1402                 1402  2017-06-30 11:00:00  \n",
      "39  94 days 03:00:00             81                   62  2017-03-07 07:00:00  \n",
      "40  26 days 09:00:00              2                    1  2017-05-04 08:00:00  \n",
      "41  86 days 23:00:00             65                   62  2017-03-07 15:00:00  \n",
      "42  85 days 05:00:00             76                   62  2017-03-07 07:00:00  \n",
      "43 212 days 09:00:00            104                   86  2017-04-24 12:00:00  \n",
      "44 147 days 21:00:00            750                  661  2017-06-27 14:00:00  \n",
      "45  86 days 05:00:00             69                   62  2017-03-07 07:00:00  \n",
      "46  84 days 08:00:00             72                   62  2017-03-07 07:00:00  \n",
      "47   0 days 00:00:00              0                    0  2017-03-07 07:00:00  \n",
      "48  83 days 07:00:00             66                   62  2017-03-07 07:00:00  \n",
      "49  85 days 02:00:00             65                   62  2017-03-07 07:00:00  \n",
      "50  85 days 11:00:00             87                   62  2017-03-07 07:00:00  \n",
      "51 152 days 01:00:00            938                  845  2017-06-23 10:00:00  \n"
     ]
    }
   ],
   "source": [
    "hourly_nans=extract_info(dwelling_ids)\n",
    "print(hourly_nans)\n",
    "hourly_nans.to_csv('Hour_NaN_Gas_Table.csv', sep='\\t', index=True,na_rep='NA')\n",
    "#Ten_secs_nans=extract_info(paths_s)\n",
    "#Ten_secs_nans.to_csv('Secs_NaN_Gas_Table.csv', sep='\\t', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
